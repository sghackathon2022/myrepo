{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sghackathon2022/myrepo/blob/main/ESG.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zTOO3kmUnYgs",
        "outputId": "f5d00884-0281-4daa-ebb5-7e65a7531033"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mPS2UptAoHNB",
        "outputId": "260899f4-ee36-4c74-db66-a61718e74ed3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (0.13.1+cu113)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torchvision) (2.23.0)\n",
            "Requirement already satisfied: torch==1.12.1 in /usr/local/lib/python3.7/dist-packages (from torchvision) (1.12.1+cu113)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torchvision) (4.1.1)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision) (7.1.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchvision) (1.21.6)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision) (2022.9.24)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision) (2.10)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.24.0-py3-none-any.whl (5.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 5.5 MB 15.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.8.0)\n",
            "Collecting huggingface-hub<1.0,>=0.10.0\n",
            "  Downloading huggingface_hub-0.11.0-py3-none-any.whl (182 kB)\n",
            "\u001b[K     |████████████████████████████████| 182 kB 44.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2022.6.2)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.13.0)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
            "  Downloading tokenizers-0.13.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 7.6 MB 43.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.10.0->transformers) (4.1.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.10.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2022.9.24)\n",
            "Installing collected packages: tokenizers, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.11.0 tokenizers-0.13.2 transformers-4.24.0\n"
          ]
        }
      ],
      "source": [
        "\n",
        "!pip install torchvision \n",
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus.reader.tagged import NLTKWordTokenizer\n",
        "import pandas as pd\n",
        "import re\n",
        "import string\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer,PorterStemmer\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "\n",
        "from nltk.tokenize import word_tokenize,sent_tokenize\n",
        "\n",
        "df=pd.read_csv('/content/drive/MyDrive/dataset1.csv')\n",
        "def clean_text(message):\n",
        "  message=message.lower()\n",
        "  message=re.sub(r'^https?:\\/\\/.*[\\r\\n]*','',message,flags=re.MULTILINE)\n",
        "  reg_pattern=re.compile(pattern=\"[\"\n",
        "                                   u\"\\U0001F600-\\U0001F64F\"\n",
        "                                   u\"\\U0001F300-\\U0001F5FF\"\n",
        "                                   u\"\\U0001F1E0-\\U0001F1FF\"\n",
        "                                   u\"\\U000026A1\"\n",
        "                                   \"]+\",flags=re.UNICODE)\n",
        "  message=reg_pattern.sub(r'',message)\n",
        "\n",
        "  words=word_tokenize(message)\n",
        "  stop_words=stopwords.words('english')\n",
        "  words=[i for i in words if i.strip() !=\"\" and i not in stop_words and i not in string.punctuation]\n",
        "  #ps=nltk.PorterStemmer()\n",
        "  wn=nltk.WordNetLemmatizer()\n",
        "  return \" \".join([wn.lemmatize(word) for word in words])\n",
        "def remove_extraspace(message):\n",
        "  message=message.lower()\n",
        "  message=re.sub(r'^https?:\\/\\/.*[\\r\\n]*','',message,flags=re.MULTILINE)\n",
        "  reg_pattern=re.compile(pattern=\"[\"\n",
        "                                   u\"\\U0001F600-\\U0001F64F\"\n",
        "                                   u\"\\U0001F300-\\U0001F5FF\"\n",
        "                                   u\"\\U0001F1E0-\\U0001F1FF\"\n",
        "                                   u\"\\U000026A1\"\n",
        "                                   \"]+\",flags=re.UNICODE)\n",
        "  message=reg_pattern.sub(r'',message)\n",
        "\n",
        "  words=word_tokenize(message)\n",
        "  return \" \".join([i for i in words if i.strip() !=\"\" ])\n",
        "\n",
        "rm_space=[]\n",
        "cln_data=[]\n",
        "for ind in df['News'].index:\n",
        "  cln_data.append(clean_text(str(df['Title'][ind])+\"\\n\"+str(df['News'][ind])))\n",
        "  rm_space.append(remove_extraspace(str(df['Title'][ind])+\"\\n\"+str(df['News'][ind])))\n",
        "df['clean_data']=cln_data\n",
        "df['data']=rm_space\n",
        "df.columns\n",
        "message=df['Title'][27]+\"\\n\"+df[\"News\"][27]\n",
        "clean_msg=df['clean_data'][27]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NNcF7sBG4Wr6",
        "outputId": "6a5544a1-ad14-4cc4-c3f9-1f654eb366e7"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertForSequenceClassification,AutoTokenizer\n",
        "import torch\n",
        "id2label= {\n",
        "    \"0\": \"None\",\n",
        "    \"1\": \"Environmental\",\n",
        "    \"2\": \"Social\",\n",
        "    \"3\": \"Governance\"\n",
        "  }\n",
        "model = BertForSequenceClassification.from_pretrained(\n",
        "    'yiyanghkust/finbert-esg', \n",
        "    num_labels = 4, #number of classifications\n",
        "   output_attentions = False, # Whether the model returns attentions weights.\n",
        "    output_hidden_states = False, # Whether the model returns all hidden-states.\n",
        ")\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model.to(device)\n",
        "tokenizer = AutoTokenizer.from_pretrained('yiyanghkust/finbert-esg')\n",
        "model.eval()\n",
        "text=\"he is good\"\n",
        "text=text.lower()\n",
        "inputs = tokenizer.encode_plus(\n",
        "            text,\n",
        "            add_special_tokens=True,\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "prediction = model(\n",
        "            inputs['input_ids'].to(device), \n",
        "            token_type_ids=inputs['token_type_ids'].to(device)\n",
        "        )[0].argmax().item()\n",
        "print(id2label[str(prediction)])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8_fsM7tV6IzP",
        "outputId": "2027224e-6b93-4066-f8df-2e02b62575fb"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Social\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MSTxWfpPoSdZ",
        "outputId": "4cff9406-0f57-4c35-fe53-71d4d1bca58f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5\n",
            "Governance-Other\n"
          ]
        }
      ],
      "source": [
        "from transformers import BertForSequenceClassification,AutoTokenizer\n",
        "import torch\n",
        "model = BertForSequenceClassification.from_pretrained(\n",
        "    '/content/drive/MyDrive/Model', \n",
        "    num_labels = 26, #number of classifications\n",
        "   output_attentions = False, # Whether the model returns attentions weights.\n",
        "    output_hidden_states = False, # Whether the model returns all hidden-states.\n",
        ")\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model.to(device)\n",
        "tokenizer = AutoTokenizer.from_pretrained('/content/drive/MyDrive/Model')\n",
        "model.eval()\n",
        "#text=\"Donard Trump taken actions against climate change but Upper house rejected his plea. \\ still he is pushing withdraw of paris climate change bill\"\n",
        "#text=text.lower()\n",
        "text=clean_msg\n",
        "inputs = tokenizer.encode_plus(\n",
        "            text,\n",
        "            add_special_tokens=True,\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "prediction = model(\n",
        "            inputs['input_ids'].to(device), \n",
        "            token_type_ids=inputs['token_type_ids'].to(device)\n",
        "        )[0].argmax().item()\n",
        "id2label= {\n",
        "    \"0\": \"Bribery & Fraud\",\n",
        "    \"1\": \"Privacy & Data Security\",\n",
        "    \"2\": \"Human Rights & Community-Other\",\n",
        "    \"3\": \"Governance-Other\",\n",
        "    \"4\": \"AntiCompetitive Practices\",\n",
        "    \"5\": \"Governance-Other\",\n",
        "    \"6\": \"Customer Relations\",\n",
        "    \"7\": \"Governance Structures\",\n",
        "    \"8\": \"Discreimination & Workforce Diversity\",\n",
        "    \"9\": \"Health & Safety\",\n",
        "    \"10\": \"Human Rights & Community\",\n",
        "    \"11\": \"Labor Management Relations\",\n",
        "    \"12\": \"Governance-Other\",\n",
        "    \"13\": \"Energy & Climate Change\",\n",
        "    \"14\": \"Product Safety & Quality\",\n",
        "    \"15\": \"Product Safety & Quality\",\n",
        "    \"16\": \"Marketing & Advertising\",\n",
        "    \"17\": \"Supply Chain Management\",\n",
        "    \"18\": \"Governance-Other\",\n",
        "    \"19\": \"Toxic Emissions & Waste/Operational Waste(Non-Hazardous)\",\n",
        "    \"20\": \"Water Stress\",\n",
        "    \"21\": \"Toxic Emissions & Waste\",\n",
        "    \"22\": \"Privacy & Data Security\",\n",
        "    \"23\": \"Biodiversity & Land Use\",\n",
        "    \"24\": \"Energy & Climate Change\",\n",
        "    \"25\": \"Energy & Climate Change\"\n",
        "  }\n",
        "print(prediction)\n",
        "print(id2label[str(prediction)])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Extract ORG with spacy\n",
        "import spacy\n",
        "import re\n",
        "NER = spacy.load(\"en_core_web_sm\")\n",
        "#raw_text=\"The Indian Space Research Organisation or is the national space agency of India, headquartered in Bengaluru. It operates under Department of Space which is directly overseen by the Prime Minister of India while Chairman of ISRO acts as executive of DOS as well.\"\n",
        "raw_text=df['Title'][27]+\"\\n\"+df[\"News\"][27]\n",
        "text1= NER(raw_text)\n",
        "spacy.explain(\"ORG\")\n",
        "com=[]\n",
        "\n",
        "for word in text1.ents:\n",
        "    if word.label_ in ['ORG']:\n",
        "      com.append(word.text)\n",
        "com=set(com)\n",
        "word_count={}\n",
        "for item in com:\n",
        "  message=raw_text\n",
        "  message=message.replace('(','')\n",
        "  item=item.replace('(','')\n",
        "  count=len(re.findall('(?='+item+')',message))\n",
        "  word_count[item]=count\n",
        "val=0\n",
        "val_key=\"\"\n",
        "for key in word_count.keys():\n",
        "  if val<int(word_count[key]):\n",
        "    val=int(word_count[key])\n",
        "    val_key=key\n",
        "\n",
        "print(val_key)"
      ],
      "metadata": {
        "id": "y0WD6Wv6wV9X",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bde3be45-3670-410e-957e-6a3ee536d989"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SBG Management\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir /content/drive/MyDrive/input_files\n",
        "import pandas as pd\n",
        "df=pd.read_csv('/content/drive/MyDrive/dataset1.csv')\n",
        "for ind in range(100):\n",
        "  open(\"/content/drive/MyDrive/input_files/\"+str(ind+1)+\".txt\",'w').write(str(df['Title'][ind]+\"\\n\"+str(df['News'][ind])))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kVxqNfBzehWT",
        "outputId": "fd51fa6d-7a5e-4dcc-a9e3-d1e067a49c50"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mkdir: cannot create directory ‘/content/drive/MyDrive/input_files’: File exists\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchvision \n",
        "!pip install transformers\n",
        "!pip install py_thesaurus\n",
        "from nltk.corpus.reader.tagged import NLTKWordTokenizer\n",
        "import pandas as pd\n",
        "import re\n",
        "import os\n",
        "import string\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "import spacy\n",
        "import re\n",
        "NER = spacy.load(\"en_core_web_sm\")\n",
        "from transformers import BertForSequenceClassification,AutoTokenizer\n",
        "import torch\n",
        "from nltk.tokenize import word_tokenize,sent_tokenize\n",
        "def clean_text(message):\n",
        "  message=message.lower()\n",
        "  message=re.sub(r'^https?:\\/\\/.*[\\r\\n]*','',message,flags=re.MULTILINE)\n",
        "  reg_pattern=re.compile(pattern=\"[\"\n",
        "                                   u\"\\U0001F600-\\U0001F64F\"\n",
        "                                   u\"\\U0001F300-\\U0001F5FF\"\n",
        "                                   u\"\\U0001F1E0-\\U0001F1FF\"\n",
        "                                   u\"\\U000026A1\"\n",
        "                                   \"]+\",flags=re.UNICODE)\n",
        "  message=reg_pattern.sub(r'',message)\n",
        "\n",
        "  words=word_tokenize(message)\n",
        "\n",
        "  stop_words=stopwords.words('english')\n",
        "  no_stop_words=[i for i in words if i.strip() !=\"\" and i not in stop_words and i not in string.punctuation]\n",
        "  #ps=nltk.PorterStemmer()\n",
        "  wn=nltk.WordNetLemmatizer()\n",
        "  return \" \".join([wn.lemmatize(word) for word in no_stop_words]),\" \".join([i for i in words])\n",
        "\n",
        "\n",
        "def get_prediction(model_name,message,size,mapping_list):\n",
        "  prediction=\"Other\"\n",
        "  try:\n",
        "    msg_512=message.split(\" \")[0:512]\n",
        "    #print(len(msg_512))\n",
        "    msg=\" \".join([i for i in msg_512])\n",
        "    \n",
        "    model = BertForSequenceClassification.from_pretrained(model_name,num_labels = size,output_attentions = False, output_hidden_states = False)\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "    inputs = tokenizer.encode_plus(message,add_special_tokens=True,return_tensors=\"pt\")\n",
        "    prediction = model(inputs['input_ids'].to(device),token_type_ids=inputs['token_type_ids'].to(device))[0].argmax().item()\n",
        "  except:\n",
        "    pass\n",
        "  return prediction\n",
        "\n",
        "def get_word_ngrams(message,phrase):\n",
        "  tokens=message.split(\" \")\n",
        "  tn_phrase=phrase.split(\" \")\n",
        "  values=[]\n",
        "  if tn_phrase[0] in tokens:\n",
        "    i=tokens.index(tn_phrase[0])\n",
        "    j=i+len(tn_phrase)+2\n",
        "    val=\"\"\n",
        "    while(i<=j and j<len(tokens)) :\n",
        "      val=val+\" \"+tokens[i]\n",
        "      i=i+1\n",
        "    i=tokens.index(tn_phrase[0])-2\n",
        "    j=tokens.index(tn_phrase[0])\n",
        "    val=\"\"\n",
        "    while(j>=i and i>=0):\n",
        "      val=val+\" \"+tokens[i]\n",
        "      i=i+1\n",
        "    val=val+\" \"+phrase\n",
        "    values.append(val.strip())\n",
        "    return list(set(values))\n",
        "    \n",
        "\n",
        "    \n",
        "def get_org_and_key_phrases(raw_text,cln_msg):\n",
        "\n",
        "  text1= NER(raw_text)\n",
        "  spacy.explain(\"ORG\")\n",
        "  com=[]\n",
        "  key_words=[]\n",
        "  for word in text1.ents:\n",
        "    if word.label_ in ['ORG']:\n",
        "      com.append(word.text)\n",
        "    elif word.label_ in ['ORDINAL','CORDINAL']:\n",
        "      key_words.append(word.text)\n",
        "  com=set(com)\n",
        "  word_count={}\n",
        "  for item in com:\n",
        "    message=raw_text\n",
        "    message=message.replace('(','')\n",
        "    if item is not None:\n",
        "      item=item.replace('(','')\n",
        "      count=len(re.findall('(?='+item+')',message))\n",
        "      word_count[item]=count\n",
        "  val=0\n",
        "  val_key=\"\"\n",
        "  for key in word_count.keys():\n",
        "    if val<int(word_count[key]):\n",
        "      val=int(word_count[key])\n",
        "      val_key=key\n",
        "  phrases=[]\n",
        "  for w in key_words:\n",
        "    vals=get_word_ngrams(cln_msg,w)\n",
        "    if vals is not None and len(vals)>=1:\n",
        "      phrases=phrases+vals\n",
        "  return val_key,phrases\n",
        "\n",
        "def is_word_exists(sentence,word):\n",
        "  ps = PorterStemmer()\n",
        "  flag=False\n",
        "  words=\" \".join([ps.stemp(str(i).lower()) for i in sentence.split(\" \")])\n",
        "  new_instance = Thesaurus(str(word).lower())\n",
        "  syn = [word]+new_instance.get_synonym()+new_instance.get_synonym(pos='verb')+new_instance.get_synonym(pos='adj')\n",
        "  for val in syn\n",
        "    val=ps.stem(val)\n",
        "    if val is not None and str(val).lower() in words :\n",
        "      i1_flag=True\n",
        "        break\n",
        "  return flag\n",
        "        \n",
        "def get_environment_nature_impact(dict1,message,type):\n",
        "  \n",
        "  sentences=sent_tokenize(message)\n",
        "  nature=\"minimal\"\n",
        "  ps = PorterStemmer()\n",
        "  for sentence in sentences:\n",
        "    for key in dict1.keys():\n",
        "      item1=dict1[key][\"object\"]\n",
        "      item2=dict1[key][\"impact\"]\n",
        "      i1_tokens=item1.split(\",\")\n",
        "      i2_tokens=item2.split(\",\")\n",
        "      i1_flag=False\n",
        "      for i1 in i1_tokens:\n",
        "        i1_flag=is_word_exists(sentence,i1)\n",
        "      i2_flag=False\n",
        "      for i2 in i2_tokens:\n",
        "        i2_flag=is_word_exists(sentence,i2)\n",
        "      if i1_flag && i2_flag:\n",
        "        nature=key\n",
        "        break\n",
        "\n",
        "  return nature\n",
        "\n",
        "def get_social_nature_impact(dict1,message,type):\n",
        "  \n",
        "  sentences=sent_tokenize(message)\n",
        "  nature=\"minimal\"\n",
        "  ps = PorterStemmer()\n",
        "  for sentence in sentences:\n",
        "    for key in [\"very serious\",\"serious\",\"medium\"]:\n",
        "      if key ==\"very serious\":\n",
        "        item3=dict1[key][\"direct_object\"]\n",
        "        for i3 in item3.split(\",\"):\n",
        "          if is_word_exists(sentence,i3.strip()):\n",
        "            nature=key\n",
        "            return nature\n",
        "      item1=dict1[key][\"object\"]\n",
        "      item2=dict1[key][\"impact\"]\n",
        "      i2_tokens=item2.split(\",\")\n",
        "      i1_flag=False\n",
        "      i3_tokens=item\n",
        "      for i1 in i1_tokens:\n",
        "        i1_flag=is_word_exists(sentence,i1.strip())\n",
        "      i2_flag=False\n",
        "      for i2 in i2_tokens:\n",
        "        i2_flag=is_word_exists(sentence,i2.strip())\n",
        "      if i1_flag && i2_flag:\n",
        "        nature=key\n",
        "        break\n",
        "\n",
        "  return nature\n",
        "\n",
        "def get_governance_nature_impact(dict1,message,type):\n",
        "  \n",
        "  sentences=sent_tokenize(message)\n",
        "  nature=\"minimal\"\n",
        "  ps = PorterStemmer()\n",
        "  for sentence in sentences:\n",
        "    for key in [\"very serious\",\"serious\",\"medium\"]:\n",
        "      item1=dict1[key][\"object\"]\n",
        "      item2=dict1[key][\"impact\"]\n",
        "      i2_tokens=item2.split(\",\")\n",
        "      i1_flag=False\n",
        "      i3_tokens=item\n",
        "      for i1 in i1_tokens:\n",
        "        i1_flag=is_word_exists(sentence,i1.strip())\n",
        "      i2_flag=False\n",
        "      for i2 in i2_tokens:\n",
        "        i2_flag=is_word_exists(sentence,i2.strip())\n",
        "      if i1_flag && i2_flag:\n",
        "        nature=key\n",
        "        break\n",
        "\n",
        "  return nature  \n",
        "\n",
        "def get_environment_nature_impact(dict1,message,type):\n",
        "  \n",
        "  sentences=sent_tokenize(message)\n",
        "  scale=\"minimal\"\n",
        "  ps = PorterStemmer()\n",
        "  for sentence in sentences:\n",
        "    for key in dict1.keys():\n",
        "      item1=dict1[key][\"object\"]\n",
        "      item2=dict1[key][\"impact\"]\n",
        "      i1_tokens=item1.split(\",\")\n",
        "      i2_tokens=item2.split(\",\")\n",
        "      i1_flag=False\n",
        "      for i1 in i1_tokens:\n",
        "        i1_flag=is_word_exists(sentence,i1)\n",
        "      i2_flag=False\n",
        "      for i2 in i2_tokens:\n",
        "        i2_flag=is_word_exists(sentence,i2)\n",
        "      if i1_flag && i2_flag:\n",
        "        scale=key\n",
        "        break\n",
        "\n",
        "  return scale\n",
        "\n",
        "def get_social_nature_impact(dict1,message,type):\n",
        "  \n",
        "  sentences=sent_tokenize(message)\n",
        "  scale=\"minimal\"\n",
        "  ps = PorterStemmer()\n",
        "  for sentence in sentences:\n",
        "    for key in [\"very serious\",\"serious\",\"medium\"]:\n",
        "      if key ==\"very serious\":\n",
        "        item3=dict1[key][\"direct_object\"]\n",
        "        for i3 in item3.split(\",\"):\n",
        "          if is_word_exists(sentence,i3.strip()):\n",
        "            nature=key\n",
        "            return nature\n",
        "      item1=dict1[key][\"object\"]\n",
        "      item2=dict1[key][\"impact\"]\n",
        "      i2_tokens=item2.split(\",\")\n",
        "      i1_flag=False\n",
        "      i3_tokens=item\n",
        "      for i1 in i1_tokens:\n",
        "        i1_flag=is_word_exists(sentence,i1.strip())\n",
        "      i2_flag=False\n",
        "      for i2 in i2_tokens:\n",
        "        i2_flag=is_word_exists(sentence,i2.strip())\n",
        "      if i1_flag && i2_flag:\n",
        "        scale=key\n",
        "        break\n",
        "\n",
        "  return scale\n",
        "\n",
        "def get_governance_nature_impact(dict1,message,type):\n",
        "  \n",
        "  sentences=sent_tokenize(message)\n",
        "  scale=\"minimal\"\n",
        "  ps = PorterStemmer()\n",
        "  for sentence in sentences:\n",
        "    for key in [\"very serious\",\"serious\",\"medium\"]:\n",
        "      item1=dict1[key][\"object\"]\n",
        "      item2=dict1[key][\"impact\"]\n",
        "      i2_tokens=item2.split(\",\")\n",
        "      i1_flag=False\n",
        "      i3_tokens=item\n",
        "      for i1 in i1_tokens:\n",
        "        i1_flag=is_word_exists(sentence,i1.strip())\n",
        "      i2_flag=False\n",
        "      for i2 in i2_tokens:\n",
        "        i2_flag=is_word_exists(sentence,i2.strip())\n",
        "      if i1_flag && i2_flag:\n",
        "        scale=key\n",
        "        break\n",
        "\n",
        "  return scale  \n",
        "\n",
        "\n",
        "\n",
        "##Execution of Code\n",
        "#df=pd.read_csv('/content/drive/MyDrive/dataset1.csv')\n",
        "esg_mapping={\n",
        "    \"0\": \"None\",\n",
        "    \"1\": \"Environment\",\n",
        "    \"2\": \"Social\",\n",
        "    \"3\": \"Governance\"\n",
        "  }\n",
        "sub_pillar_mapping={\n",
        "    \"0\": \"Bribery & Fraud\",\n",
        "    \"1\": \"Privacy & Data Security\",\n",
        "    \"2\": \"Human Rights & Community-Other\",\n",
        "    \"3\": \"Governance-Other\",\n",
        "    \"4\": \"AntiCompetitive Practices\",\n",
        "    \"5\": \"Governance-Other\",\n",
        "    \"6\": \"Customer Relations\",\n",
        "    \"7\": \"Governance Structures\",\n",
        "    \"8\": \"Discreimination & Workforce Diversity\",\n",
        "    \"9\": \"Health & Safety\",\n",
        "    \"10\": \"Human Rights & Community\",\n",
        "    \"11\": \"Labor Management Relations\",\n",
        "    \"12\": \"Governance-Other\",\n",
        "    \"13\": \"Energy & Climate Change\",\n",
        "    \"14\": \"Product Safety & Quality\",\n",
        "    \"15\": \"Product Safety & Quality\",\n",
        "    \"16\": \"Marketing & Advertising\",\n",
        "    \"17\": \"Supply Chain Management\",\n",
        "    \"18\": \"Governance-Other\",\n",
        "    \"19\": \"Toxic Emissions & Waste/Operational Waste(Non-Hazardous)\",\n",
        "    \"20\": \"Water Stress\",\n",
        "    \"21\": \"Toxic Emissions & Waste\",\n",
        "    \"22\": \"Privacy & Data Security\",\n",
        "    \"23\": \"Biodiversity & Land Use\",\n",
        "    \"24\": \"Energy & Climate Change\",\n",
        "    \"25\": \"Energy & Climate Change\"\n",
        "  }\n",
        "nature={\"environment\":{\n",
        "              \"very serious\":{\"object\":\"plant, wildlife, habitat, ecosystem\",\"impact\":\"death, destruction\"},\n",
        "              \"serious\":{\"object\":\"plant, wildlife, habitat, ecosystem\",\"impact\":\"debilitation, injury, illness, major damage\"},\n",
        "              \"medium\":{\"object\":\"plant, wildlife, habitat, ecosystem, pollution\",\"impact\":\"damage, short-term damage\"},\n",
        "              \"minimal\":{\"object\":\"\",\"impact\":\"\"}},\n",
        "  \"social\":{\n",
        "              \"very serious\":{\"object\":\"human rights, livelihood, life, traditional life, traditional way of life, property\",\"direct_object\":\"death,crimes, disability, permanent disability,torture, rape, enslavement\",\"impact\":\"violation, destruction,serious\"},\n",
        "              \"serious\":{\"object\":\"injury,illness,property, livelihood, life, traditional life, traditional way of life, labor rights, labour rights, civil rights, privacy, collective bargaining\",\"impact\":\"debilitating, damage, impairment, displacement, violation, harm\"},\n",
        "              \"medium\":{\"object\":\"injury, illness, property, livelihood, life, traditional life, traditional way of life\",\"impact\":\"treatable, short-term, short term, temporary, non-serious, light, slight\"},\n",
        "              \"minimal\":{\"object\":\"\",\"impact\":\"\"}},\n",
        "           \n",
        "  \"governance\":{\n",
        "              \"very serious\":{\"object\":\"national government, economy, bribe, loss, company loss, contracts, contracts value, gains, illegal gains, profit, dishonest profit, corrupt gains, tax, income tax\",\"impact\":\"destabilise, substantially destabilised, illegal, dishonest, corrupt, avoided, USD 1 billion,  USD 10 billion\"},\n",
        "              \"serious\":{\"object\":\"company, non-government customer, private customer, government body, public sector, government organisation, pension funds, pension, bribe, contracts\",\"impact\":\"bankruptcy, USD 100 million, USD 5 billion\"},\n",
        "              \"medium\":{\"object\":\"company, B2B, property, government fraud\",\"impact\":\"corruption, fraud, damage, non-serious, slight, light,  \"}],\n",
        "              \"minimal\": {\"object\":\"\",\"impact\":\"\"}}\n",
        "}\n",
        "\n",
        "\n",
        "#Added Extra two columns have data with stop words and with out stop words\n",
        "esg_data=[]\n",
        "pillar_data=[]\n",
        "org_data=[]\n",
        "key_phrases=[]\n",
        "esg_model='yiyanghkust/finbert-esg'\n",
        "pillar_model='/content/drive/MyDrive/Model'\n",
        "files=[]\n",
        "data=[]\n",
        "\n",
        "for file in os.listdir('/content/drive/MyDrive/input_files'):\n",
        "    # check if current path is a file\n",
        "    if os.path.isfile(os.path.join('/content/drive/MyDrive/input_files', file)):\n",
        "        files.append(os.path.join('/content/drive/MyDrive/input_files', file))\n",
        "for file in files:\n",
        "  print(file)\n",
        "  msg=open(file,\"r\").read()\n",
        "  data.append(msg)\n",
        "  msg=msg.replace(\"(\",\"\").replace(\")\",\"\")\n",
        "  cln_msg,full_msg=clean_text(msg)\n",
        "\n",
        "  esg_data.append(get_prediction(esg_model,cln_msg,4,esg_mapping))\n",
        "\n",
        "  pillar_data.append(get_prediction(pillar_model,cln_msg,4,sub_pillar_mapping))\n",
        "  org,phrases=get_org_and_key_phrases(full_msg,cln_msg)\n",
        "  org_data.append(org)\n",
        "  key_phrases.append(phrases)\n",
        "df=pd.DataFrame(data,columns=[\"data\"])\n",
        "df['key_phrases']=key_phrases \n",
        "df['esg_type']=esg_data\n",
        "df['sub_pillar']=pillar_data\n",
        "df['org']=org_data\n",
        "df2=df[['data','esg_type','sub_pillar','org']]\n",
        "json=df2.to_json()\n",
        "df2.to_csv('output.csv')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6f60f573-5ee6-43c1-d1c4-3ca48f61599d",
        "id": "_AnCVld202Wy"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (0.13.1+cu113)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision) (7.1.2)\n",
            "Requirement already satisfied: torch==1.12.1 in /usr/local/lib/python3.7/dist-packages (from torchvision) (1.12.1+cu113)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torchvision) (2.23.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchvision) (1.21.6)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torchvision) (4.1.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision) (2022.9.24)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision) (3.0.4)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.24.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.10.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.11.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.8.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.13.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2022.6.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.13.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.10.0->transformers) (4.1.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.10.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2022.9.24)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/input_files/1.txt\n",
            "/content/drive/MyDrive/input_files/2.txt\n",
            "/content/drive/MyDrive/input_files/3.txt\n",
            "/content/drive/MyDrive/input_files/4.txt\n",
            "/content/drive/MyDrive/input_files/5.txt\n",
            "/content/drive/MyDrive/input_files/6.txt\n",
            "/content/drive/MyDrive/input_files/7.txt\n",
            "/content/drive/MyDrive/input_files/8.txt\n",
            "/content/drive/MyDrive/input_files/9.txt\n",
            "/content/drive/MyDrive/input_files/10.txt\n",
            "/content/drive/MyDrive/input_files/11.txt\n",
            "/content/drive/MyDrive/input_files/12.txt\n",
            "/content/drive/MyDrive/input_files/13.txt\n",
            "/content/drive/MyDrive/input_files/14.txt\n",
            "/content/drive/MyDrive/input_files/15.txt\n",
            "/content/drive/MyDrive/input_files/16.txt\n",
            "/content/drive/MyDrive/input_files/17.txt\n",
            "/content/drive/MyDrive/input_files/18.txt\n",
            "/content/drive/MyDrive/input_files/19.txt\n",
            "/content/drive/MyDrive/input_files/20.txt\n",
            "/content/drive/MyDrive/input_files/21.txt\n",
            "/content/drive/MyDrive/input_files/22.txt\n",
            "/content/drive/MyDrive/input_files/23.txt\n",
            "/content/drive/MyDrive/input_files/24.txt\n",
            "/content/drive/MyDrive/input_files/25.txt\n",
            "/content/drive/MyDrive/input_files/26.txt\n",
            "/content/drive/MyDrive/input_files/27.txt\n",
            "/content/drive/MyDrive/input_files/28.txt\n",
            "/content/drive/MyDrive/input_files/29.txt\n",
            "/content/drive/MyDrive/input_files/30.txt\n",
            "/content/drive/MyDrive/input_files/31.txt\n",
            "/content/drive/MyDrive/input_files/32.txt\n",
            "/content/drive/MyDrive/input_files/33.txt\n",
            "/content/drive/MyDrive/input_files/34.txt\n",
            "/content/drive/MyDrive/input_files/35.txt\n",
            "/content/drive/MyDrive/input_files/36.txt\n",
            "/content/drive/MyDrive/input_files/37.txt\n",
            "/content/drive/MyDrive/input_files/38.txt\n",
            "/content/drive/MyDrive/input_files/39.txt\n",
            "/content/drive/MyDrive/input_files/40.txt\n",
            "/content/drive/MyDrive/input_files/41.txt\n",
            "/content/drive/MyDrive/input_files/42.txt\n",
            "/content/drive/MyDrive/input_files/43.txt\n",
            "/content/drive/MyDrive/input_files/44.txt\n",
            "/content/drive/MyDrive/input_files/45.txt\n",
            "/content/drive/MyDrive/input_files/46.txt\n",
            "/content/drive/MyDrive/input_files/47.txt\n",
            "/content/drive/MyDrive/input_files/48.txt\n",
            "/content/drive/MyDrive/input_files/49.txt\n",
            "/content/drive/MyDrive/input_files/50.txt\n",
            "/content/drive/MyDrive/input_files/51.txt\n",
            "/content/drive/MyDrive/input_files/52.txt\n",
            "/content/drive/MyDrive/input_files/53.txt\n",
            "/content/drive/MyDrive/input_files/54.txt\n",
            "/content/drive/MyDrive/input_files/55.txt\n",
            "/content/drive/MyDrive/input_files/56.txt\n",
            "/content/drive/MyDrive/input_files/57.txt\n",
            "/content/drive/MyDrive/input_files/58.txt\n",
            "/content/drive/MyDrive/input_files/59.txt\n",
            "/content/drive/MyDrive/input_files/60.txt\n",
            "/content/drive/MyDrive/input_files/61.txt\n",
            "/content/drive/MyDrive/input_files/62.txt\n",
            "/content/drive/MyDrive/input_files/63.txt\n",
            "/content/drive/MyDrive/input_files/64.txt\n",
            "/content/drive/MyDrive/input_files/65.txt\n",
            "/content/drive/MyDrive/input_files/66.txt\n",
            "/content/drive/MyDrive/input_files/67.txt\n",
            "/content/drive/MyDrive/input_files/68.txt\n",
            "/content/drive/MyDrive/input_files/69.txt\n",
            "/content/drive/MyDrive/input_files/70.txt\n",
            "/content/drive/MyDrive/input_files/71.txt\n",
            "/content/drive/MyDrive/input_files/72.txt\n",
            "/content/drive/MyDrive/input_files/73.txt\n",
            "/content/drive/MyDrive/input_files/74.txt\n",
            "/content/drive/MyDrive/input_files/75.txt\n",
            "/content/drive/MyDrive/input_files/76.txt\n",
            "/content/drive/MyDrive/input_files/77.txt\n",
            "/content/drive/MyDrive/input_files/78.txt\n",
            "/content/drive/MyDrive/input_files/79.txt\n",
            "/content/drive/MyDrive/input_files/80.txt\n",
            "/content/drive/MyDrive/input_files/81.txt\n",
            "/content/drive/MyDrive/input_files/82.txt\n",
            "/content/drive/MyDrive/input_files/83.txt\n",
            "/content/drive/MyDrive/input_files/84.txt\n",
            "/content/drive/MyDrive/input_files/85.txt\n",
            "/content/drive/MyDrive/input_files/86.txt\n",
            "/content/drive/MyDrive/input_files/87.txt\n",
            "/content/drive/MyDrive/input_files/88.txt\n",
            "/content/drive/MyDrive/input_files/89.txt\n",
            "/content/drive/MyDrive/input_files/90.txt\n",
            "/content/drive/MyDrive/input_files/91.txt\n",
            "/content/drive/MyDrive/input_files/92.txt\n",
            "/content/drive/MyDrive/input_files/93.txt\n",
            "/content/drive/MyDrive/input_files/94.txt\n",
            "/content/drive/MyDrive/input_files/95.txt\n",
            "/content/drive/MyDrive/input_files/96.txt\n",
            "/content/drive/MyDrive/input_files/97.txt\n",
            "/content/drive/MyDrive/input_files/98.txt\n",
            "/content/drive/MyDrive/input_files/99.txt\n",
            "/content/drive/MyDrive/input_files/100.txt\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1M63QiH_DuIurA_boe-fsz8-59IRoSvqN",
      "authorship_tag": "ABX9TyND4qKwiSDihlD21LEVWua2",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}