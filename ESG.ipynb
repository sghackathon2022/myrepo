{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sghackathon2022/myrepo/blob/main/ESG.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zTOO3kmUnYgs",
        "outputId": "d8571ead-f2ab-4403-b7e2-81512dea4dbe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mPS2UptAoHNB",
        "outputId": "b6b1aa6d-7a38-41bd-a541-5b9216fe3bab"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (0.13.1+cu113)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torchvision) (2.23.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision) (7.1.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchvision) (1.21.6)\n",
            "Requirement already satisfied: torch==1.12.1 in /usr/local/lib/python3.7/dist-packages (from torchvision) (1.12.1+cu113)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torchvision) (4.1.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision) (2022.9.24)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision) (2.10)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.24.0-py3-none-any.whl (5.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 5.5 MB 5.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.13.0)\n",
            "Collecting huggingface-hub<1.0,>=0.10.0\n",
            "  Downloading huggingface_hub-0.11.0-py3-none-any.whl (182 kB)\n",
            "\u001b[K     |████████████████████████████████| 182 kB 63.1 MB/s \n",
            "\u001b[?25hCollecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
            "  Downloading tokenizers-0.13.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 7.6 MB 37.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2022.6.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.8.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.10.0->transformers) (4.1.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.10.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2022.9.24)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Installing collected packages: tokenizers, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.11.0 tokenizers-0.13.2 transformers-4.24.0\n"
          ]
        }
      ],
      "source": [
        "\n",
        "!pip install torchvision \n",
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus.reader.tagged import NLTKWordTokenizer\n",
        "import pandas as pd\n",
        "import re\n",
        "import string\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer,PorterStemmer\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "\n",
        "from nltk.tokenize import word_tokenize,sent_tokenize\n",
        "\n",
        "df=pd.read_csv('/content/drive/MyDrive/dataset1.csv')\n",
        "def clean_text(message):\n",
        "  message=message.lower()\n",
        "  message=re.sub(r'^https?:\\/\\/.*[\\r\\n]*','',message,flags=re.MULTILINE)\n",
        "  reg_pattern=re.compile(pattern=\"[\"\n",
        "                                   u\"\\U0001F600-\\U0001F64F\"\n",
        "                                   u\"\\U0001F300-\\U0001F5FF\"\n",
        "                                   u\"\\U0001F1E0-\\U0001F1FF\"\n",
        "                                   u\"\\U000026A1\"\n",
        "                                   \"]+\",flags=re.UNICODE)\n",
        "  message=reg_pattern.sub(r'',message)\n",
        "\n",
        "  words=word_tokenize(message)\n",
        "  stop_words=stopwords.words('english')\n",
        "  words=[i for i in words if i.strip() !=\"\" and i not in stop_words and i not in string.punctuation]\n",
        "  #ps=nltk.PorterStemmer()\n",
        "  wn=nltk.WordNetLemmatizer()\n",
        "  return \" \".join([wn.lemmatize(word) for word in words])\n",
        "def remove_extraspace(message):\n",
        "  message=message.lower()\n",
        "  message=re.sub(r'^https?:\\/\\/.*[\\r\\n]*','',message,flags=re.MULTILINE)\n",
        "  reg_pattern=re.compile(pattern=\"[\"\n",
        "                                   u\"\\U0001F600-\\U0001F64F\"\n",
        "                                   u\"\\U0001F300-\\U0001F5FF\"\n",
        "                                   u\"\\U0001F1E0-\\U0001F1FF\"\n",
        "                                   u\"\\U000026A1\"\n",
        "                                   \"]+\",flags=re.UNICODE)\n",
        "  message=reg_pattern.sub(r'',message)\n",
        "\n",
        "  words=word_tokenize(message)\n",
        "  return \" \".join([i for i in words if i.strip() !=\"\" ])\n",
        "\n",
        "rm_space=[]\n",
        "cln_data=[]\n",
        "for ind in df['News'].index:\n",
        "  cln_data.append(clean_text(str(df['Title'][ind])+\"\\n\"+str(df['News'][ind])))\n",
        "  rm_space.append(remove_extraspace(str(df['Title'][ind])+\"\\n\"+str(df['News'][ind])))\n",
        "df['clean_data']=cln_data\n",
        "df['data']=rm_space\n",
        "df.columns\n",
        "message=df['Title'][27]+\"\\n\"+df[\"News\"][27]\n",
        "clean_msg=df['clean_data'][27]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NNcF7sBG4Wr6",
        "outputId": "ce06016d-d758-46cd-91de-3a84564c2cea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data1=\"there were allegations of sexual assault against reliance \"\n",
        "data2=\"I love you\"\n",
        "from transformers import AutoModelForSequenceClassification,AutoTokenizer,pipeline\n",
        "sent_model = AutoModelForSequenceClassification.from_pretrained(\"yiyanghkust/finbert-tone\", num_labels=3)\n",
        "sent_tokenizer = AutoTokenizer.from_pretrained(\"yiyanghkust/finbert-tone\")\n",
        "sent_nlp=pipeline(\"text-classification\",model=sent_model,tokenizer=sent_tokenizer)\n",
        "dict1={\"LABEL_0\":\"NEUTRAL\",\"LABEL_1\":\"POSITIVE\",\"LABEL_2\":\"NEGATIVE\"}\n",
        "#data=dict1[sent_nlp(df['clean_data'][0])[0][\"label\"]]\n",
        "#data2=dict1[sent_nlp(\"I hate you\")[0][\"label\"]]\n",
        "print(sent_nlp(data1)[0],data1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bVvfrkRjyiHQ",
        "outputId": "f25f708b-1e2c-4082-b9e2-9916bc8bc3c3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'label': 'Negative', 'score': 0.9687488079071045} there were allegations of sexual assault against reliance \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertForSequenceClassification,AutoTokenizer,pipeline\n",
        "import torch\n",
        "id2label= {\n",
        "    \"0\": \"None\",\n",
        "    \"1\": \"Environmental\",\n",
        "    \"2\": \"Social\",\n",
        "    \"3\": \"Governance\"\n",
        "  }\n",
        "esg_model = AutoModelForSequenceClassification.from_pretrained(\"yiyanghkust/finbert-esg\", num_labels=4)\n",
        "esg_tokenizer = AutoTokenizer.from_pretrained(\"yiyanghkust/finbert-esg\")\n",
        "esg_nlp=pipeline(\"text-classification\",model=esg_model,tokenizer=esg_tokenizer)\n",
        "print(esg_nlp(\"Reliance continued to face controversies related to allegations of negligence in its health care facilities across the US. In relation to this, it recently faced allegations of sexual assault in its Viirginia facility with lawsuits from former minor patients filed against the company, hospital and former physiotherapist.\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8_fsM7tV6IzP",
        "outputId": "1df31eab-e69d-4419-de59-c01699635cb4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'label': 'Social', 'score': 0.6581017971038818}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus.reader.tagged import NLTKWordTokenizer\n",
        "import pandas as pd\n",
        "import re\n",
        "import string\n",
        "import nltk\n",
        "from nltk.corpus import stopwords,wordnet\n",
        "from nltk.stem import WordNetLemmatizer,PorterStemmer,SnowballStemmer\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "import spacy\n",
        "NER = spacy.load(\"en_core_web_sm\")\n",
        "from nltk.tokenize import word_tokenize,sent_tokenize\n",
        "def is_word_exists(sentence,keyword):\n",
        "  \n",
        "  ps = SnowballStemmer(\"english\")\n",
        "  wn=WordNetLemmatizer()\n",
        "  words=\" \".join([ps.stem(str(i).lower()) for i in word_tokenize(sentence)]) +\" \".join([str(i).lower() for i in word_tokenize(sentence)])+\" \".join([wn.lemmatize(str(i).lower()) for i in word_tokenize(sentence)])\n",
        "  #print(words)\n",
        "  \n",
        "  synonyms=[]\n",
        "  tokens=keyword.split(\" \")\n",
        "  vals=[]\n",
        "  for token in tokens:\n",
        "    flag=False\n",
        "    for syn in wordnet.synsets(token):\n",
        "      for lm in syn.lemmas():\n",
        "              synonyms.append(lm.name())\n",
        "    syn = [token]+synonyms\n",
        "    ps_syn=syn+[ps.stem(w) for w in syn] +[wn.lemmatize(f) for f in syn]\n",
        "    syn=set(ps_syn)\n",
        "    for word in syn:\n",
        "      if word is not None and str(word).lower() in words :\n",
        "        flag=True\n",
        "        break\n",
        "    vals.append(flag)\n",
        "  return len(tokens) == len([f for f in vals if f is True])\n",
        "\n",
        "def get_nature_impact(dict1,message,type):\n",
        "  sentences=sent_tokenize(message)\n",
        "  nature=\"minimal\"\n",
        "  for sentence in sentences:\n",
        "    for key in [\"very serious\",\"serious\",\"medium\"]:\n",
        "\n",
        "      if key ==\"very serious\" and type == \"social\":\n",
        "        item3=dict1[key][\"direct_object\"]\n",
        "        for i3 in item3.split(\",\"):\n",
        "          if is_word_exists(sentence,i3.strip()):\n",
        "            nature=key\n",
        "            return nature\n",
        "      item1=dict1[key][\"object\"]\n",
        "      item2=dict1[key][\"impact\"]\n",
        "      i1_tokens=item2.split(\",\")\n",
        "      i2_tokens=item2.split(\",\")\n",
        "      i1_flag=False\n",
        "      \n",
        "      for i1 in i1_tokens:\n",
        "        i1_flag=is_word_exists(sentence,i1.strip())\n",
        "        if(i1_flag):\n",
        "          break\n",
        "      i2_flag=False\n",
        "      for i2 in i2_tokens:\n",
        "        if any(char.isdigit() for char in i2.strip()):\n",
        "            docs= NER(sentence)\n",
        "            for doc in docs.ents:\n",
        "              if doc.label_ == \"MONEY\":\n",
        "                d=doc.text\n",
        "\n",
        "                tokens=sentence.split(\" \")\n",
        "                k=0\n",
        "                d1=\"\"\n",
        "                while (k<len(tokens)):\n",
        "                  if d in tokens[k]:\n",
        "                    d1=tokens[k]+tokens[k+1]\n",
        "                    break\n",
        "                  k=k+1\n",
        "                    \n",
        "                num=int(\"\".join([i for i in d1 if i.isdigit() or i==\".\" ]))\n",
        "                if \"million\" in d or \"mn\" in d or \"m\" in d1 :\n",
        "                   num=num*1000000\n",
        "                elif \"billion\" in d or \"bn\" in d or \"b\" in d1:\n",
        "                  num=num*1000000000\n",
        "                d=num\n",
        "\n",
        "                if key == \"very serious\" and d>=1000000000:\n",
        "                  i2_flag=True\n",
        " \n",
        "                elif key == \"serious\" and d>=100000000 and d<1000000000:\n",
        "                  i2_flag=True\n",
        "\n",
        "                elif key == \"medium\" and d<100000000:\n",
        "                  i2_flag=True\n",
        "        else:\n",
        "          i2_flag=is_word_exists(sentence,i2.strip())\n",
        "          if (i2_flag):\n",
        "            break\n",
        "      if i1_flag and i2_flag:\n",
        "          return key\n",
        "  return nature\n",
        "\n",
        "nature_dict={\"environment\":{\n",
        "              \"very serious\":{\"object\":\"plant, wildlife, habitat, ecosystem\",\"impact\":\"death,dead, destruction,destroy\"},\n",
        "              \"serious\":{\"object\":\"plant, wildlife, habitat, ecosystem\",\"impact\":\"debilitation, injury,'injuri', illness, major damage\"},\n",
        "              \"medium\":{\"object\":\"plant, wildlife, habitat, ecosystem, pollution\",\"impact\":\"damage, short term damage\"},\n",
        "              \"minimal\":{\"object\":\"\",\"impact\":\"\"}},\n",
        "  \"social\":{\n",
        "              \"very serious\":{\"object\":\"human rights, livelihood, life, traditional life, traditional way of life, property\",\"direct_object\":\"death,crimes, dead,disability, permanent disability,torture, rape, enslavement\",\"impact\":\"violation, destruction,serious,destroy\"},\n",
        "              \"serious\":{\"object\":\"property,people,mass,employee,livelihood, life,traditional life, traditional way of life, labor rights, labour rights, civil rights, privacy, collective bargaining\",\"impact\":\"debilitating, damage, impairment, displacement, violation, harm,injuri,injury,illness,risk\"},\n",
        "              \"medium\":{\"object\":\"injury,'injuri', illness, property, livelihood, life, traditional life, traditional way of life\",\"impact\":\"treatable, short-term, short term, temporary, non-serious, light, slight\"},\n",
        "              \"minimal\":{\"object\":\"\",\"impact\":\"\"}},\n",
        "           \n",
        "  \"governance\":{\n",
        "              \"very serious\":{\"object\":\"national government, economy, bribe,unlawfully, loss, company loss, contracts, contracts value, gains, illegal gains, profit, dishonest profit, corrupt gains, tax, income tax\",\"impact\":\"destabilise, substantially destabilised, illegal, dishonest, corrupt, avoided, USD 1000000000,  USD 10000000000\"},\n",
        "              \"serious\":{\"object\":\"company, non-government customer, unlawfully,private customer, government body, public sector, government organisation, pension funds, pension, bribe, contracts\",\"impact\":\"bankruptcy, USD 100000000, USD 5000000000\"},\n",
        "              \"medium\":{\"object\":\"company, B2B, property, government fraud\",\"impact\":\"corruption, fraud, damage, non-serious, slight, light\"},\n",
        "              \"minimal\": {\"object\":\"\",\"impact\":\"\"}}\n",
        "}\n",
        "data1=\"Fire accident happened yesterday night in african forest.1M Animals death and habitat destruction done\"\n",
        "data2=\"People got fired from infosys organization due to dollar to rupee conversion low in india\"\n",
        "data3=\"destabilized happened over year in pakistan and goverment taking minimal actions which effects country economy growth\"\n",
        "print(get_nature_impact(nature_dict[\"environment\"],data1,\"environment\"))\n",
        "print(get_nature_impact(nature_dict[\"social\"],data2,\"social\"))\n",
        "print(get_nature_impact(nature_dict[\"governance\"],data2,\"governance\"))\n",
        "print(get_nature_impact(nature_dict[\"social\"],\"Shell's factory emitted poisonous gases that led to serious health risk to its employees\",\"social\"))\n",
        "print(get_nature_impact(nature_dict[\"social\"],\"Reliance continued to face controversies related to allegations of negligence in its health care facilities across the US. In relation to this, it recently faced allegations of sexual assault in its Viirginia facility with lawsuits from former minor patients filed against the company, hospital and former physiotherapist.\",\"social\"))\n",
        "print(get_nature_impact(nature_dict[\"governance\"],\"The CEO of Shell and his associates were accused of accepting money unlawfully to the tune of $123 mn\",\"governance\"))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RWdH891U4afh",
        "outputId": "4831d62c-1bf3-4f54-9c58-91d72f800d83"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "very serious\n",
            "minimal\n",
            "minimal\n",
            "very serious\n",
            "very serious\n",
            "minimal\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus.reader.tagged import NLTKWordTokenizer\n",
        "import pandas as pd\n",
        "import re\n",
        "import string\n",
        "import nltk\n",
        "from nltk.corpus import stopwords,wordnet\n",
        "from nltk.stem import WordNetLemmatizer,PorterStemmer,SnowballStemmer\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "import spacy\n",
        "NER = spacy.load(\"en_core_web_sm\")\n",
        "from nltk.tokenize import word_tokenize,sent_tokenize\n",
        "def is_word_exists(sentence,keyword):\n",
        "  #print(keyword)\n",
        "  ps = SnowballStemmer(\"english\")\n",
        "  wn=WordNetLemmatizer()\n",
        "  words=\" \".join([ps.stem(str(i).lower()) for i in word_tokenize(sentence)]) +\" \".join([str(i).lower() for i in word_tokenize(sentence)])+\" \".join([wn.lemmatize(str(i).lower()) for i in word_tokenize(sentence)])\n",
        "  #print(words)\n",
        "\n",
        "  synonyms=[]\n",
        "  k1=keyword.split(\" \")\n",
        "  vals=[]\n",
        "  for token in k1:\n",
        "    token=token.strip()\n",
        "    flag=False\n",
        "    for syn in wordnet.synsets(token):\n",
        "      for lm in syn.lemmas():\n",
        "              synonyms.append(lm.name())\n",
        "    syn = [token]+synonyms\n",
        "    ps_syn=syn+[ps.stem(w) for w in syn] +[wn.lemmatize(w) for w in syn]\n",
        "    syn=syn+ps_syn\n",
        "    if \"unlawfully\" in syn :\n",
        "        print(\"word1\") \n",
        "    for word in syn:\n",
        "      \n",
        "      if word is not None and str(word).lower() in words :\n",
        "        flag=True\n",
        "        break\n",
        "    vals.append(flag)\n",
        "  return len(k1) == len([f for f in vals if f is True])\n",
        "\n",
        "def get_scale_impact(dict1,message):\n",
        "  sentences=sent_tokenize(message)\n",
        "  scale=\"minimal\"\n",
        "  for sentence in sentences:\n",
        "    for key in [\"extremely widespread\",\"extensive\",\"limited\"]:\n",
        "      for item in dict1[key]:\n",
        "        item1=item[\"object\"]\n",
        "        item2=item[\"quantity\"]\n",
        "        i1_tokens=item1.split(\",\")\n",
        "        i2_tokens=item2.split(\",\")\n",
        "        i1_flag=False\n",
        "        for i1 in i1_tokens:\n",
        "          i1_flag=is_word_exists(sentence,i1.strip())\n",
        "          if(i1_flag):\n",
        "            break\n",
        "\n",
        "        i2_flag=False\n",
        "        for i2 in i2_tokens:\n",
        "          if any(char.isdigit() for char in i2.strip()):\n",
        "            docs= NER(sentence)\n",
        "            for doc in docs.ents:\n",
        "              if doc.label_ in [\"MONEY\",\"CARDINAL\"]:\n",
        "                d=doc.text\n",
        "              tokens=sentence.split(\" \")\n",
        "              k=0\n",
        "              d1=\"\"\n",
        "              while (k<len(tokens)):\n",
        "                if d in tokens[k]:\n",
        "                  if \"km2\" in tokens[k+1] :\n",
        "                    d1=tokens[k]\n",
        "                  elif k<len(tokens)-1:\n",
        "                    d1=tokens[k]+tokens[k+1]\n",
        "                  else:\n",
        "                    d1=tokens[k]\n",
        "                  break\n",
        "                k=k+1\n",
        "              #print(d1)  \n",
        "              num=int(\"\".join([i for i in d1 if i.isdigit() or i==\".\" ]))\n",
        "              if \"million\" in d1 or \"mn\" in d1 :\n",
        "                num=num*1000000\n",
        "              elif \"billion\" in d1 or \"bn\" in d1:\n",
        "                num=num*1000000000\n",
        "              d=num\n",
        "              if (\">\" in i2.strip() or \">=\" in i2.strip()) and (\"<\" not in i2.strip() or \"between\" not in i2.strip()):\n",
        "                if \">=\" in i2.strip():\n",
        "                  if d>=int(\"\".join([i for i in i2.strip() if i.isdigit()])):\n",
        "                    i2_flag=True\n",
        "                else:\n",
        "                  if d>int(\"\".join([i for i in i2.strip() if i.isdigit()])):\n",
        "                    i2_flag=True\n",
        "\n",
        "              elif ((\">\" in i2.strip()) and (\"<\" in i2.strip()) or \"between\" in i2.strip()):\n",
        "                if \"between\" in i2.strip():\n",
        "                  vals=i2.strip().split(\" \")\n",
        "                  low=int(\"\".join([i for i in vals[vals.index(\"between\")+1] if i.isdigit()]))\n",
        "                  high=int(\"\".join([i for i in vals[vals.index(\"and\")+1] if i.isdigit()]))\n",
        "                  if d>=low and d<=high:\n",
        "                    i2_flag=True\n",
        "              else:\n",
        "                \n",
        "                if d< int(\"\".join([i for i in i2.strip() if i.isdigit()])):\n",
        "                  i2_flag=True\n",
        "              for key_token in i2.strip().split(\" \"):\n",
        "                if i2_flag and key_token not in [\"between\",\"and\"] and not any(char.isdigit() for char in key_token):\n",
        "                  i2_flag=is_word_exists(sentence,key_token)\n",
        "              if i1_flag and i2_flag:\n",
        "                return key\n",
        "            else:\n",
        "              i2_flag=is_word_exists(sentence,i2.strip())\n",
        "              if i1_flag and i2_flag:\n",
        "                return key\n",
        "      \n",
        "  return scale  \n",
        "scale_dict ={\n",
        "  \"environment\":{\n",
        "              \"extremely widespread\":[{\"object\":\"watershed, sea, ocean\",\"quantity\":\">=100 km2, >=100 square kilometre, >=100 square kilometer, >=100 sqkm\"},{\"object\":\"species,wildlife\",\"quantity\":\"global\"},{\"object\":\"barrels, barrels spilled\",\"quantity\":\">=60000\"},{\"object\":\"contributors, company\",\"quantity\":\"=<10, top 10\"},{\"object\":\"duration\",\"quantity\":\">5 years\"},{\"object\":\"sovereign states, countries\",\"quantity\":\"many, several, multiple\"}],\n",
        "              \"extensive\": [{\"object\":\"large bay, river\",\"quantity\":\"between 10 and 99 km2, between 10 and 99 sqkm, between 10 and 99 square kilometre, between 10 and 99 square kilometer\"},{\"object\":\"species, wildlife\",\"quantity\":\"regional, country\"},{\"object\":\"barrels, barrels spilled\",\"quantity\":\"between 5000 and 59999\"},{\"object\":\"contributors, company\",\"quantity\":\"one of many\"}],\n",
        "              \"limited\": [{\"object\":\"water stream, small river, lake, pond\",\"quantity\":\"between 1 and 9 km2, between 1 and 9 sqkm, between 1 and 9 square kilometre, between 1 and 9 square kilometer\"},{\"object\":\"species, wildlife\",\"quantity\":\"local\"},{\"object\":\"barrels, barrels spilled\",\"quantity\":\"between 1500 and 4999\"}],\n",
        "              \"low\":[ {\"object\":\"\",\"quantity\":\"\"}]},\n",
        "   \"social\":{\n",
        "              \"extremely widespread\":[{\"object\":\"people\",\"quantity\":\">=1000\"},{\"object\":\"properties\",\"quantity\":\">=2000\"},{\"object\":\"contributors, company\",\"quantity\":\"<10, top 10\"},{\"object\":\"impact, duration\",\"quantity\":\"long-lasting, >5 years\"},{\"object\":\"economic harm, monetary, economic, financial impact, financial\",\"quantity\":\">=USD 10 bn, >=USD 10 billion\"}],\n",
        "              \"extensive\": [{\"object\":\"people\",\"quantity\":\"between 25 and 999\"},{\"object\":\"properties\",\"quantity\":\"between 100 and 1999\"},{\"object\":\"contributors, company\",\"quantity\":\"one of many\"}],\n",
        "              \"limited\": [{\"object\":\"people\",\"quantity\":\"between 10 and 24\"},{\"object\":\"properties\",\"quantity\":\"between 10 and 99\"}],\n",
        "              \"low\":[ {\"object\":\"\",\"quantity\":\"\"}]},          \n",
        "  \"governance\":{\n",
        "             \"extremely widespread\":[{\"object\":\"people\",\"quantity\":\">1000\"},{\"object\":\"impact\",\"quantity\":\"global\"},{\"object\":\" Argentina, Australia, Brazil, Canada, China, France, Germany, India, Indonesia, Italy, Japan, Korea, Mexico, Russia, Saudi Arabia, South Africa, Turkey, United Kingdom, UK, Britain, England, United States, European Union \",\"quantity\":\"count>3\"},{\"object\":\"impact, duration\",\"quantity\":\"long-lasting, >5 years\"},{\"object\":\"economic harm, monetary, economic, financial impact, financial\",\"quantity\":\">USD 10 bn, >USD 10 billion\"}],\n",
        "              \"extensive\": [{\"object\":\"people\",\"quantity\":\"between 25 and 1000\"}, {\"object\":\" Argentina, Australia, Brazil, Canada, China, France, Germany, India, Indonesia, Italy, Japan, Korea, Mexico, Russia, Saudi Arabia, South Africa, Turkey, United Kingdom, UK, Britain, England, United States, European Union \",\"quantity\":\"count between 1 and 3\"},{\"object\":\"other countries, countries\",\"quantity\":\">=3\"}],\n",
        "              \"limited\": [{\"object\":\"people\",\"quantity\":\"between 10 and 24\"},{\"object\":\"other countries, countries\",\"quantity\":\"between 1 and 2\"},{\"object\":\"minicipalt, local municipalty\",\"quantity\":\" between 1 and 2\"}],\n",
        "              \"low\":[ {\"object\":\"\",\"quantity\":\"\"}]}\n",
        "}\n",
        "#data1=\"Fire accident happened yesterday night in african forest.1M Animals death and habitat destruction done\"\n",
        "#data2=\"People got fired from infosys organization due to dollar to rupee conversion low in india\"\n",
        "#data3=\"destabilized happened over year in pakistan and goverment taking minimal actions which effects country economy growth\"\n",
        "t=get_scale_impact(scale_dict[\"environment\"],\"small river spread of 9 sqkm\")\n",
        "print(t)\n",
        "#print(get_social_scale_impact(nature_dict[\"social\"],\"Reliance continued to face controversies related to allegations of negligence in its health care facilities across the US. In relation to this, it recently faced allegations of sexual assault in its Viirginia facility with lawsuits from former minor patients filed against the company, hospital and former physiotherapist.\"))\n",
        "#print(get_governance_scale_impact(nature_dict[\"governance\"],\"The CEO of Shell and his associates were accused of accepting money unlawfully to the tune of $123 mn\"))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ba363a2b-0c86-4189-b892-8d0d3fb73578",
        "id": "3Z9oRpCJeD4m"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "limited\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "id": "MSTxWfpPoSdZ",
        "outputId": "3c88c7d6-956c-467e-f7c6-f441417f768f"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-2a4dd1e47187>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBertForSequenceClassification\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mAutoTokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m model = BertForSequenceClassification.from_pretrained(\n\u001b[1;32m      4\u001b[0m     \u001b[0;34m'/content/drive/MyDrive/Model'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mnum_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m26\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;31m#number of classifications\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'transformers'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "from transformers import BertForSequenceClassification,AutoTokenizer\n",
        "import torch\n",
        "model = BertForSequenceClassification.from_pretrained(\n",
        "    '/content/drive/MyDrive/Model', \n",
        "    num_labels = 26, #number of classifications\n",
        "   output_attentions = False, # Whether the model returns attentions weights.\n",
        "    output_hidden_states = False, # Whether the model returns all hidden-states.\n",
        ")\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model.to(device)\n",
        "tokenizer = AutoTokenizer.from_pretrained('/content/drive/MyDrive/Model')\n",
        "model.eval()\n",
        "#text=\"Donard Trump taken actions against climate change but Upper house rejected his plea. \\ still he is pushing withdraw of paris climate change bill\"\n",
        "#text=text.lower()\n",
        "text=\"there were allegations of sexual assault against reliance \"\n",
        "#text=clean_msg\n",
        "inputs = tokenizer.encode_plus(\n",
        "            text,\n",
        "            add_special_tokens=True,\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "prediction = model(\n",
        "            inputs['input_ids'].to(device), \n",
        "            token_type_ids=inputs['token_type_ids'].to(device)\n",
        "        )[0].argmax().item()\n",
        "id2label= {\n",
        "    \"0\": \"Bribery & Fraud\",\n",
        "    \"1\": \"Privacy & Data Security\",\n",
        "    \"2\": \"Human Rights & Community-Other\",\n",
        "    \"3\": \"Governance-Other\",\n",
        "    \"4\": \"AntiCompetitive Practices\",\n",
        "    \"5\": \"Governance-Other\",\n",
        "    \"6\": \"Customer Relations\",\n",
        "    \"7\": \"Governance Structures\",\n",
        "    \"8\": \"Discrimination & Workforce Diversity\",\n",
        "    \"9\": \"Health & Safety\",\n",
        "    \"10\": \"Human Rights & Community\",\n",
        "    \"11\": \"Labor Management Relations\",\n",
        "    \"12\": \"Governance-Other\",\n",
        "    \"13\": \"Energy & Climate Change\",\n",
        "    \"14\": \"Product Safety & Quality\",\n",
        "    \"15\": \"Product Safety & Quality\",\n",
        "    \"16\": \"Marketing & Advertising\",\n",
        "    \"17\": \"Supply Chain Management\",\n",
        "    \"18\": \"Governance-Other\",\n",
        "    \"19\": \"Toxic Emissions & Waste/Operational Waste(Non-Hazardous)\",\n",
        "    \"20\": \"Water Stress\",\n",
        "    \"21\": \"Toxic Emissions & Waste\",\n",
        "    \"22\": \"Privacy & Data Security\",\n",
        "    \"23\": \"Biodiversity & Land Use\",\n",
        "    \"24\": \"Energy & Climate Change\",\n",
        "    \"25\": \"Energy & Climate Change\"\n",
        "  }\n",
        "print(prediction)\n",
        "print(id2label[str(prediction)])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Extract ORG with spacy\n",
        "import spacy\n",
        "import re\n",
        "NER = spacy.load(\"en_core_web_sm\")\n",
        "#raw_text=\"The Indian Space Research Organisation or is the national space agency of India, headquartered in Bengaluru. It operates under Department of Space which is directly overseen by the Prime Minister of India while Chairman of ISRO acts as executive of DOS as well.\"\n",
        "raw_text=df['Title'][27]+\"\\n\"+df[\"News\"][27]\n",
        "text1= NER(raw_text)\n",
        "spacy.explain(\"ORG\")\n",
        "com=[]\n",
        "\n",
        "for word in text1.ents:\n",
        "    if word.label_ in ['ORG']:\n",
        "      com.append(word.text)\n",
        "com=set(com)\n",
        "word_count={}\n",
        "for item in com:\n",
        "  message=raw_text\n",
        "  message=message.replace('(','')\n",
        "  item=item.replace('(','')\n",
        "  count=len(re.findall('(?='+item+')',message))\n",
        "  word_count[item]=count\n",
        "val=0\n",
        "val_key=\"\"\n",
        "for key in word_count.keys():\n",
        "  if val<int(word_count[key]):\n",
        "    val=int(word_count[key])\n",
        "    val_key=key\n",
        "\n",
        "print(val_key)"
      ],
      "metadata": {
        "id": "y0WD6Wv6wV9X",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bde3be45-3670-410e-957e-6a3ee536d989"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SBG Management\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir /content/drive/MyDrive/input_files\n",
        "import pandas as pd\n",
        "df=pd.read_csv('/content/drive/MyDrive/dataset1.csv')\n",
        "for ind in range(100):\n",
        "  open(\"/content/drive/MyDrive/inpp\"+str(ind+1)+\".txt\",'w').write(str(df['Title'][ind]+\"\\n\"+str(df['News'][ind])))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kVxqNfBzehWT",
        "outputId": "fd51fa6d-7a5e-4dcc-a9e3-d1e067a49c50"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mkdir: cannot create directory ‘/content/drive/MyDrive/input_files’: File exists\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus.reader import documents\n",
        "!pip install torchvision \n",
        "!pip install transformers\n",
        "!pip install py_thesaurus\n",
        "from nltk.corpus.reader.tagged import NLTKWordTokenizer\n",
        "import pandas as pd\n",
        "import re\n",
        "import os\n",
        "import string\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer,PorterStemmer,SnowballStemmer\n",
        "from nltk.tokenize import word_tokenize,sent_tokenize\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "import spacy\n",
        "import re\n",
        "NER = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "from transformers import AutoModelForSequenceClassification,AutoTokenizer,pipeline\n",
        "\n",
        "import torch\n",
        "from nltk.tokenize import word_tokenize,sent_tokenize\n",
        "def clean_text(message):\n",
        "  message=message.lower()\n",
        "  message=re.sub(r'^https?:\\/\\/.*[\\r\\n]*','',message,flags=re.MULTILINE)\n",
        "  reg_pattern=re.compile(pattern=\"[\"\n",
        "                                   u\"\\U0001F600-\\U0001F64F\"\n",
        "                                   u\"\\U0001F300-\\U0001F5FF\"\n",
        "                                   u\"\\U0001F1E0-\\U0001F1FF\"\n",
        "                                   u\"\\U000026A1\"\n",
        "                                   \"]+\",flags=re.UNICODE)\n",
        "  message=reg_pattern.sub(r'',message)\n",
        "\n",
        "  words=word_tokenize(message)\n",
        "\n",
        "  stop_words=stopwords.words('english')\n",
        "  no_stop_words=[i for i in words if i.strip() !=\"\" and i not in stop_words and i not in string.punctuation]\n",
        "  #ps=nltk.PorterStemmer()\n",
        "  wn=nltk.WordNetLemmatizer()\n",
        "  return \" \".join([wn.lemmatize(word) for word in no_stop_words]),\" \".join([i for i in words])\n",
        "\n",
        "\n",
        "def get_prediction(model_name,message,size,mapping_list):\n",
        "  prediction=\"Other\"\n",
        "  try:\n",
        "    msg_512=message.split(\" \")[0:512]\n",
        "    #print(len(msg_512))\n",
        "    msg=\" \".join([i for i in msg_512])\n",
        "    \n",
        "    model = BertForSequenceClassification.from_pretrained(model_name,num_labels = size,output_attentions = False, output_hidden_states = False)\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "    inputs = tokenizer.encode_plus(message,add_special_tokens=True,return_tensors=\"pt\")\n",
        "    prediction = model(inputs['input_ids'].to(device),token_type_ids=inputs['token_type_ids'].to(device))[0].argmax().item()\n",
        "  except:\n",
        "    pass\n",
        "  return prediction\n",
        "\n",
        "def get_word_ngrams(message,phrase):\n",
        "  tokens=message.split(\" \")\n",
        "  tn_phrase=phrase.split(\" \")\n",
        "  values=[]\n",
        "  if tn_phrase[0] in tokens:\n",
        "    i=tokens.index(tn_phrase[0])\n",
        "    j=i+len(tn_phrase)+2\n",
        "    val=\"\"\n",
        "    while(i<=j and j<len(tokens)) :\n",
        "      val=val+\" \"+tokens[i]\n",
        "      i=i+1\n",
        "    i=tokens.index(tn_phrase[0])-2\n",
        "    j=tokens.index(tn_phrase[0])\n",
        "    val=\"\"\n",
        "    while(j>=i and i>=0):\n",
        "      val=val+\" \"+tokens[i]\n",
        "      i=i+1\n",
        "    val=val+\" \"+phrase\n",
        "    values.append(val.strip())\n",
        "    return list(set(values))\n",
        "    \n",
        "\n",
        "    \n",
        "def get_org_and_key_phrases(raw_text,cln_msg):\n",
        "\n",
        "  text1= NER(raw_text)\n",
        "  spacy.explain(\"ORG\")\n",
        "  com=[]\n",
        "  key_words=[]\n",
        "  for word in text1.ents:\n",
        "    if word.label_ in ['ORG']:\n",
        "      com.append(word.text)\n",
        "    elif word.label_ in ['ORDINAL','CORDINAL']:\n",
        "      key_words.append(word.text)\n",
        "  com=set(com)\n",
        "  word_count={}\n",
        "  for item in com:\n",
        "    message=raw_text\n",
        "    message=message.replace('(','')\n",
        "    if item is not None:\n",
        "      item=item.replace('(','')\n",
        "      count=len(re.findall('(?='+item+')',message))\n",
        "      word_count[item]=count\n",
        "  val=0\n",
        "  val_key=\"\"\n",
        "  for key in word_count.keys():\n",
        "    if val<int(word_count[key]):\n",
        "      val=int(word_count[key])\n",
        "      val_key=key\n",
        "  phrases=[]\n",
        "  for w in key_words:\n",
        "    vals=get_word_ngrams(cln_msg,w)\n",
        "    if vals is not None and len(vals)>=1:\n",
        "      phrases=phrases+vals\n",
        "  return val_key,phrases\n",
        "\n",
        "def is_word_exists(sentence,keyword):\n",
        "  \n",
        "  ps = SnowballStemmer(\"english\")\n",
        "  wn=WordNetLemmatizer()\n",
        "  words=\" \".join([ps.stem(str(i).lower()) for i in word_tokenize(sentence)]) +\" \".join([str(i).lower() for i in word_tokenize(sentence)])+\" \".join([wn.lemmatize(str(i).lower()) for i in word_tokenize(sentence)])\n",
        "  #print(words)\n",
        "  \n",
        "  synonyms=[]\n",
        "  tokens=keyword.split(\" \")\n",
        "  vals=[]\n",
        "  for token in tokens:\n",
        "    flag=False\n",
        "    for syn in wordnet.synsets(token):\n",
        "      for lm in syn.lemmas():\n",
        "              synonyms.append(lm.name())\n",
        "    syn = [token]+synonyms\n",
        "    ps_syn=syn+[ps.stem(w) for w in syn] +[wn.lemmatize(f) for f in syn]\n",
        "    syn=set(ps_syn)\n",
        "    for word in syn:\n",
        "      if word is not None and str(word).lower() in words :\n",
        "        flag=True\n",
        "        break\n",
        "    vals.append(flag)\n",
        "  return len(tokens) == len([f for f in vals if f is True])\n",
        "        \n",
        "def get_nature_impact(dict1,message,type):\n",
        "  sentences=sent_tokenize(message)\n",
        "  nature=\"minimal\"\n",
        "  for sentence in sentences:\n",
        "    for key in [\"very serious\",\"serious\",\"medium\"]:\n",
        "\n",
        "      if key ==\"very serious\" and type == \"social\":\n",
        "        item3=dict1[key][\"direct_object\"]\n",
        "        for i3 in item3.split(\",\"):\n",
        "          if is_word_exists(sentence,i3.strip()):\n",
        "            nature=key\n",
        "            return nature\n",
        "      item1=dict1[key][\"object\"]\n",
        "      item2=dict1[key][\"impact\"]\n",
        "      i1_tokens=item2.split(\",\")\n",
        "      i2_tokens=item2.split(\",\")\n",
        "      i1_flag=False\n",
        "      \n",
        "      for i1 in i1_tokens:\n",
        "        i1_flag=is_word_exists(sentence,i1.strip())\n",
        "        if(i1_flag):\n",
        "          break\n",
        "      i2_flag=False\n",
        "      for i2 in i2_tokens:\n",
        "        if any(char.isdigit() for char in i2.strip()):\n",
        "            docs= NER(sentence)\n",
        "            for doc in docs.ents:\n",
        "              if doc.label_ == \"MONEY\":\n",
        "                d=doc.text\n",
        "\n",
        "                tokens=sentence.split(\" \")\n",
        "                k=0\n",
        "                d1=\"\"\n",
        "                while (k<len(tokens)):\n",
        "                  if d in tokens[k]:\n",
        "                    d1=tokens[k]+tokens[k+1]\n",
        "                    break\n",
        "                  k=k+1\n",
        "                    \n",
        "                num=int(\"\".join([i for i in d1 if i.isdigit() or i==\".\" ]))\n",
        "                if \"million\" in d or \"mn\" in d or \"m\" in d1 :\n",
        "                   num=num*1000000\n",
        "                elif \"billion\" in d or \"bn\" in d or \"b\" in d1:\n",
        "                  num=num*1000000000\n",
        "                d=num\n",
        "\n",
        "                if key == \"very serious\" and d>=1000000000:\n",
        "                  i2_flag=True\n",
        " \n",
        "                elif key == \"serious\" and d>=100000000 and d<1000000000:\n",
        "                  i2_flag=True\n",
        "\n",
        "                elif key == \"medium\" and d<100000000:\n",
        "                  i2_flag=True\n",
        "        else:\n",
        "          i2_flag=is_word_exists(sentence,i2.strip())\n",
        "          if (i2_flag):\n",
        "            break\n",
        "      if i1_flag and i2_flag:\n",
        "          return key\n",
        "  return nature\n",
        "nature_dict={\"environment\":{\n",
        "              \"very serious\":{\"object\":\"plant, wildlife, habitat, ecosystem\",\"impact\":\"death,dead, destruction,destroy\"},\n",
        "              \"serious\":{\"object\":\"plant, wildlife, habitat, ecosystem\",\"impact\":\"debilitation, injury,'injuri', illness, major damage\"},\n",
        "              \"medium\":{\"object\":\"plant, wildlife, habitat, ecosystem, pollution\",\"impact\":\"damage, short term damage\"},\n",
        "              \"minimal\":{\"object\":\"\",\"impact\":\"\"}},\n",
        "  \"social\":{\n",
        "              \"very serious\":{\"object\":\"human rights, livelihood, life, traditional life, traditional way of life, property\",\"direct_object\":\"death,crimes, dead,disability, permanent disability,torture, rape, enslavement\",\"impact\":\"violation, destruction,serious,destroy\"},\n",
        "              \"serious\":{\"object\":\"property,people,mass,employee,livelihood, life,traditional life, traditional way of life, labor rights, labour rights, civil rights, privacy, collective bargaining\",\"impact\":\"debilitating, damage, impairment, displacement, violation, harm,injuri,injury,illness,risk\"},\n",
        "              \"medium\":{\"object\":\"injury,'injuri', illness, property, livelihood, life, traditional life, traditional way of life\",\"impact\":\"treatable, short-term, short term, temporary, non-serious, light, slight\"},\n",
        "              \"minimal\":{\"object\":\"\",\"impact\":\"\"}},\n",
        "           \n",
        "  \"governance\":{\n",
        "              \"very serious\":{\"object\":\"national government, economy, bribe,unlawfully, loss, company loss, contracts, contracts value, gains, illegal gains, profit, dishonest profit, corrupt gains, tax, income tax\",\"impact\":\"destabilise, substantially destabilised, illegal, dishonest, corrupt, avoided, USD 1000000000,  USD 10000000000\"},\n",
        "              \"serious\":{\"object\":\"company, non-government customer, unlawfully,private customer, government body, public sector, government organisation, pension funds, pension, bribe, contracts\",\"impact\":\"bankruptcy, USD 100000000, USD 5000000000\"},\n",
        "              \"medium\":{\"object\":\"company, B2B, property, government fraud\",\"impact\":\"corruption, fraud, damage, non-serious, slight, light\"},\n",
        "              \"minimal\": {\"object\":\"\",\"impact\":\"\"}}\n",
        "}\n",
        "def get_scale(dict1,message):\n",
        "  sentences=sent_tokenize(message)\n",
        "  scale=\"minimal\"\n",
        "  for sentence in sentences:\n",
        "    for key in [\"extremely widespread\",\"extensive\",\"limited\"]:\n",
        "      for item in dict1[key]:\n",
        "        item1=item[\"object\"]\n",
        "        item2=item[\"quantity\"]\n",
        "        i1_tokens=item1.split(\",\")\n",
        "        i2_tokens=item2.split(\",\")\n",
        "        i1_flag=False\n",
        "        for i1 in i1_tokens:\n",
        "          i1_flag=is_word_exists(sentence,i1.strip())\n",
        "          if(i1_flag):\n",
        "            break\n",
        "\n",
        "        i2_flag=False\n",
        "        for i2 in i2_tokens:\n",
        "          if any(char.isdigit() for char in i2.strip()):\n",
        "            docs= NER(sentence)\n",
        "            for doc in docs.ents:\n",
        "              if doc.label_ in [\"MONEY\",\"CARDINAL\"]:\n",
        "                d=doc.text\n",
        "              tokens=sentence.split(\" \")\n",
        "              k=0\n",
        "              d1=\"\"\n",
        "              while (k<len(tokens)):\n",
        "                if d in tokens[k]:\n",
        "                  if \"km2\" in tokens[k+1] :\n",
        "                    d1=tokens[k]\n",
        "                  elif k<len(tokens)-1:\n",
        "                    d1=tokens[k]+tokens[k+1]\n",
        "                  else:\n",
        "                    d1=tokens[k]\n",
        "                  break\n",
        "                k=k+1\n",
        "              #print(d1)  \n",
        "              num=int(\"\".join([i for i in d1 if i.isdigit() or i==\".\" ]))\n",
        "              if \"million\" in d1 or \"mn\" in d1 :\n",
        "                num=num*1000000\n",
        "              elif \"billion\" in d1 or \"bn\" in d1:\n",
        "                num=num*1000000000\n",
        "              d=num\n",
        "              if (\">\" in i2.strip() or \">=\" in i2.strip()) and (\"<\" not in i2.strip() or \"between\" not in i2.strip()):\n",
        "                if \">=\" in i2.strip():\n",
        "                  if d>=int(\"\".join([i for i in i2.strip() if i.isdigit()])):\n",
        "                    i2_flag=True\n",
        "                else:\n",
        "                  if d>int(\"\".join([i for i in i2.strip() if i.isdigit()])):\n",
        "                    i2_flag=True\n",
        "\n",
        "              elif ((\">\" in i2.strip()) and (\"<\" in i2.strip()) or \"between\" in i2.strip()):\n",
        "                if \"between\" in i2.strip():\n",
        "                  vals=i2.strip().split(\" \")\n",
        "                  low=int(\"\".join([i for i in vals[vals.index(\"between\")+1] if i.isdigit()]))\n",
        "                  high=int(\"\".join([i for i in vals[vals.index(\"and\")+1] if i.isdigit()]))\n",
        "                  if d>=low and d<=high:\n",
        "                    i2_flag=True\n",
        "              else:\n",
        "                \n",
        "                if d< int(\"\".join([i for i in i2.strip() if i.isdigit()])):\n",
        "                  i2_flag=True\n",
        "              for key_token in i2.strip().split(\" \"):\n",
        "                if i2_flag and key_token not in [\"between\",\"and\"] and not any(char.isdigit() for char in key_token):\n",
        "                  i2_flag=is_word_exists(sentence,key_token)\n",
        "              if i1_flag and i2_flag:\n",
        "                return key\n",
        "            else:\n",
        "              i2_flag=is_word_exists(sentence,i2.strip())\n",
        "              if i1_flag and i2_flag:\n",
        "                return key\n",
        "      \n",
        "  return scale  \n",
        "scale_dict ={\n",
        "  \"environment\":{\n",
        "              \"extremely widespread\":[{\"object\":\"watershed, sea, ocean\",\"quantity\":\">=100 km2, >=100 square kilometre, >=100 square kilometer, >=100 sqkm\"},{\"object\":\"species,wildlife\",\"quantity\":\"global\"},{\"object\":\"barrels, barrels spilled\",\"quantity\":\">=60000\"},{\"object\":\"contributors, company\",\"quantity\":\"=<10, top 10\"},{\"object\":\"duration\",\"quantity\":\">5 years\"},{\"object\":\"sovereign states, countries\",\"quantity\":\"many, several, multiple\"}],\n",
        "              \"extensive\": [{\"object\":\"large bay, river\",\"quantity\":\"between 10 and 99 km2, between 10 and 99 sqkm, between 10 and 99 square kilometre, between 10 and 99 square kilometer\"},{\"object\":\"species, wildlife\",\"quantity\":\"regional, country\"},{\"object\":\"barrels, barrels spilled\",\"quantity\":\"between 5000 and 59999\"},{\"object\":\"contributors, company\",\"quantity\":\"one of many\"}],\n",
        "              \"limited\": [{\"object\":\"water stream, small river, lake, pond\",\"quantity\":\"between 1 and 9 km2, between 1 and 9 sqkm, between 1 and 9 square kilometre, between 1 and 9 square kilometer\"},{\"object\":\"species, wildlife\",\"quantity\":\"local\"},{\"object\":\"barrels, barrels spilled\",\"quantity\":\"between 1500 and 4999\"}],\n",
        "              \"low\":[ {\"object\":\"\",\"quantity\":\"\"}]},\n",
        "   \"social\":{\n",
        "              \"extremely widespread\":[{\"object\":\"people\",\"quantity\":\">=1000\"},{\"object\":\"properties\",\"quantity\":\">=2000\"},{\"object\":\"contributors, company\",\"quantity\":\"<10, top 10\"},{\"object\":\"impact, duration\",\"quantity\":\"long-lasting, >5 years\"},{\"object\":\"economic harm, monetary, economic, financial impact, financial\",\"quantity\":\">=USD 10 bn, >=USD 10 billion\"}],\n",
        "              \"extensive\": [{\"object\":\"people\",\"quantity\":\"between 25 and 999\"},{\"object\":\"properties\",\"quantity\":\"between 100 and 1999\"},{\"object\":\"contributors, company\",\"quantity\":\"one of many\"}],\n",
        "              \"limited\": [{\"object\":\"people\",\"quantity\":\"between 10 and 24\"},{\"object\":\"properties\",\"quantity\":\"between 10 and 99\"}],\n",
        "              \"low\":[ {\"object\":\"\",\"quantity\":\"\"}]},          \n",
        "  \"governance\":{\n",
        "             \"extremely widespread\":[{\"object\":\"people\",\"quantity\":\">1000\"},{\"object\":\"impact\",\"quantity\":\"global\"},{\"object\":\" Argentina, Australia, Brazil, Canada, China, France, Germany, India, Indonesia, Italy, Japan, Korea, Mexico, Russia, Saudi Arabia, South Africa, Turkey, United Kingdom, UK, Britain, England, United States, European Union \",\"quantity\":\"count>3\"},{\"object\":\"impact, duration\",\"quantity\":\"long-lasting, >5 years\"},{\"object\":\"economic harm, monetary, economic, financial impact, financial\",\"quantity\":\">USD 10 bn, >USD 10 billion\"}],\n",
        "              \"extensive\": [{\"object\":\"people\",\"quantity\":\"between 25 and 1000\"}, {\"object\":\" Argentina, Australia, Brazil, Canada, China, France, Germany, India, Indonesia, Italy, Japan, Korea, Mexico, Russia, Saudi Arabia, South Africa, Turkey, United Kingdom, UK, Britain, England, United States, European Union \",\"quantity\":\"count between 1 and 3\"},{\"object\":\"other countries, countries\",\"quantity\":\">=3\"}],\n",
        "              \"limited\": [{\"object\":\"people\",\"quantity\":\"between 10 and 24\"},{\"object\":\"other countries, countries\",\"quantity\":\"between 1 and 2\"},{\"object\":\"minicipalt, local municipalty\",\"quantity\":\" between 1 and 2\"}],\n",
        "              \"low\":[ {\"object\":\"\",\"quantity\":\"\"}]}\n",
        "}\n",
        "\n",
        "\n",
        "\n",
        "##Execution of Code\n",
        "#df=pd.read_csv('/content/drive/MyDrive/dataset1.csv')\n",
        "sent_mapping={\"LABEL_0\":\"NEGATIVE\",\"LABEL_1\":\"POSITIVE\"}\n",
        "esg_mapping={\n",
        "    \"0\": \"None\",\n",
        "    \"1\": \"Environment\",\n",
        "    \"2\": \"Social\",\n",
        "    \"3\": \"Governance\"\n",
        "  }\n",
        "sub_pillar_mapping={\n",
        "    \"0\": \"Bribery & Fraud\",\n",
        "    \"1\": \"Privacy & Data Security\",\n",
        "    \"2\": \"Human Rights & Community-Other\",\n",
        "    \"3\": \"Governance-Other\",\n",
        "    \"4\": \"AntiCompetitive Practices\",\n",
        "    \"5\": \"Governance-Other\",\n",
        "    \"6\": \"Customer Relations\",\n",
        "    \"7\": \"Governance Structures\",\n",
        "    \"8\": \"Discreimination & Workforce Diversity\",\n",
        "    \"9\": \"Health & Safety\",\n",
        "    \"10\": \"Human Rights & Community\",\n",
        "    \"11\": \"Labor Management Relations\",\n",
        "    \"12\": \"Governance-Other\",\n",
        "    \"13\": \"Energy & Climate Change\",\n",
        "    \"14\": \"Product Safety & Quality\",\n",
        "    \"15\": \"Product Safety & Quality\",\n",
        "    \"16\": \"Marketing & Advertising\",\n",
        "    \"17\": \"Supply Chain Management\",\n",
        "    \"18\": \"Governance-Other\",\n",
        "    \"19\": \"Toxic Emissions & Waste/Operational Waste(Non-Hazardous)\",\n",
        "    \"20\": \"Water Stress\",\n",
        "    \"21\": \"Toxic Emissions & Waste\",\n",
        "    \"22\": \"Privacy & Data Security\",\n",
        "    \"23\": \"Biodiversity & Land Use\",\n",
        "    \"24\": \"Energy & Climate Change\",\n",
        "    \"25\": \"Energy & Climate Change\"\n",
        "  }\n",
        "label2id= {\n",
        "    \"Access_And_Affordability\": 2,\n",
        "    \"Air_Quality\": 21,\n",
        "    \"Business_Ethics\": 0,\n",
        "    \"Business_Model_Resilience\": 3,\n",
        "    \"Competitive_Behavior\": 4,\n",
        "    \"Critical_Incident_Risk_Management\": 5,\n",
        "    \"Customer_Privacy\": 22,\n",
        "    \"Customer_Welfare\": 6,\n",
        "    \"Data_Security\": 1,\n",
        "    \"Director_Removal\": 7,\n",
        "    \"Ecological_Impacts\": 23,\n",
        "    \"Employee_Engagement_Inclusion_And_Diversity\": 8,\n",
        "    \"Employee_Health_And_Safety\": 9,\n",
        "    \"Energy_Management\": 24,\n",
        "    \"GHG_Emissions\": 25,\n",
        "    \"Human_Rights_And_Community_Relations\": 10,\n",
        "    \"Labor_Practices\": 11,\n",
        "    \"Management_Of_Legal_And_Regulatory_Framework\": 12,\n",
        "    \"Physical_Impacts_Of_Climate_Change\": 13,\n",
        "    \"Product_Design_And_Lifecycle_Management\": 15,\n",
        "    \"Product_Quality_And_Safety\": 14,\n",
        "    \"Selling_Practices_And_Product_Labeling\": 16,\n",
        "    \"Supply_Chain_Management\": 17,\n",
        "    \"Systemic_Risk_Management\": 18,\n",
        "    \"Waste_And_Hazardous_Materials_Management\": 19,\n",
        "    \"Water_And_Wastewater_Management\": 20\n",
        "  }\n",
        "from transformers import AutoModelForSequenceClassification,AutoTokenizer,pipeline\n",
        "sent_model = AutoModelForSequenceClassification.from_pretrained(\"yiyanghkust/finbert-tone\", num_labels=3)\n",
        "sent_tokenizer = AutoTokenizer.from_pretrained(\"yiyanghkust/finbert-tone\")\n",
        "sent_nlp=pipeline(\"text-classification\",model=sent_model,tokenizer=sent_tokenizer)\n",
        "esg_model = AutoModelForSequenceClassification.from_pretrained('yiyanghkust/finbert-esg', num_labels=4)\n",
        "esg_tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
        "esg_nlp=pipeline(\"text-classification\",model=esg_model,tokenizer=esg_tokenizer)\n",
        "\n",
        "pillar_model = AutoModelForSequenceClassification.from_pretrained('nbroad/ESG-BERT', num_labels=26)\n",
        "pillar_tokenizer = AutoTokenizer.from_pretrained(\"nbroad/ESG-BERT\")\n",
        "pillar_nlp=pipeline(\"text-classification\",model=pillar_model,tokenizer=pillar_tokenizer)\n",
        "#Added Extra two columns have data with stop words and with out stop words\n",
        "esg_data=[]\n",
        "pillar_data=[]\n",
        "org_data=[]\n",
        "key_phrases=[]\n",
        "files=[]\n",
        "data=[]\n",
        "controversy=[]\n",
        "nature=[]\n",
        "scale=[]\n",
        "for file in os.listdir('/content/drive/MyDrive/input_files'):\n",
        "    # check if current path is a file\n",
        "    if os.path.isfile(os.path.join('/content/drive/MyDrive/input_files', file)):\n",
        "        files.append(os.path.join('/content/drive/MyDrive/input_files', file))\n",
        "for file in files:\n",
        "  print(file)\n",
        "  msg=open(file,\"r\").read()\n",
        "  data.append(msg)\n",
        "  msg=msg.replace(\"(\",\"\").replace(\")\",\"\")\n",
        "  cln_msg,full_msg=clean_text(msg)\n",
        "  cln_msg=\" \".join([w for w in cln_msg.split(\" \")[0:512]])\n",
        "\n",
        "  try:\n",
        "    controversy.append(sent_nlp(cln_msg)[\"label\"])\n",
        "  except:\n",
        "    controversy.append(\"NEUTRAL\")\n",
        "  try:\n",
        "    \n",
        "    val=esg_mapping[esg_nlp(cln_msg)[0][\"label\"]].lower()\n",
        "    if val == \"environment\":\n",
        "      nature.append(get_nature_impact(nature_dict[val],msg,\"environment\"))\n",
        "      \n",
        "    elif val == \"social\":\n",
        "      nature.append(get_nature_impact(nature_dict[val],msg,\"social\"))\n",
        "    elif val == \"governance\":\n",
        "      nature.append(get_nature_impact(nature_dict[val],msg,\"governance\"))\n",
        "    esg_data.append(val)\n",
        "    scale.append(get_scale(scale_dict[val],msg))\n",
        "  except:\n",
        "     esg_data.append(\"None\")\n",
        "     nature.append(\" \")\n",
        "     scale.append(\" \")\n",
        "  try:\n",
        "    pillar1=pillar_nlp(cln_msg)[0][\"label\"]  \n",
        "    print(pillar1)\n",
        "    pillar_data.append(sub_pillar_mapping[label2id[pillar1]])\n",
        "  except:\n",
        "     pillar_data.append(\"None\") \n",
        "  org,phrases=get_org_and_key_phrases(full_msg,cln_msg)\n",
        "  org_data.append(org)\n",
        "  key_phrases.append(phrases)\n",
        "df=pd.DataFrame(data,columns=[\"data\"])\n",
        "df['controversy']=controversy\n",
        "df['key_phrases']=key_phrases \n",
        "df['esg_type']=esg_data\n",
        "df['sub_pillar']=pillar_data\n",
        "df['org']=org_data\n",
        "df['nature']=nature\n",
        "df['scale']=scale\n",
        "df2=df[['data','controversy','esg_type','nature','sub_pillar','org','key_phrases']]\n",
        "json1=df2.to_json()\n",
        "open(\"output.json\",\"w\").write(json1)\n",
        "df2.to_csv('output.csv')"
      ],
      "metadata": {
        "id": "dxsFdSkN1T79",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c75dbe0f-d105-41ea-f8d3-3e9dfbb0c44d"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (0.13.1+cu113)\n",
            "Requirement already satisfied: torch==1.12.1 in /usr/local/lib/python3.7/dist-packages (from torchvision) (1.12.1+cu113)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torchvision) (4.1.1)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision) (7.1.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torchvision) (2.23.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchvision) (1.21.6)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision) (2022.9.24)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.24.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2022.6.2)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.13.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.13.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.8.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.10.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.11.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.10.0->transformers) (4.1.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.10.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2022.9.24)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: py_thesaurus in /usr/local/lib/python3.7/dist-packages (1.0.5)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.7/dist-packages (from py_thesaurus) (4.6.3)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.7/dist-packages (from py_thesaurus) (4.9.1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/input_files/1.txt\n",
            "Labor_Practices\n",
            "/content/drive/MyDrive/input_files/2.txt\n",
            "Management_Of_Legal_And_Regulatory_Framework\n",
            "/content/drive/MyDrive/input_files/3.txt\n",
            "Business_Ethics\n",
            "/content/drive/MyDrive/input_files/4.txt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Token indices sequence length is longer than the specified maximum sequence length for this model (812 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (812 > 512). Running this sequence through the model will result in indexing errors\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Employee_Engagement_Inclusion_And_Diversity\n",
            "/content/drive/MyDrive/input_files/5.txt\n",
            "/content/drive/MyDrive/input_files/6.txt\n",
            "Competitive_Behavior\n",
            "/content/drive/MyDrive/input_files/7.txt\n",
            "/content/drive/MyDrive/input_files/8.txt\n",
            "Physical_Impacts_Of_Climate_Change\n",
            "/content/drive/MyDrive/input_files/9.txt\n",
            "Energy_Management\n",
            "/content/drive/MyDrive/input_files/10.txt\n",
            "Employee_Health_And_Safety\n",
            "/content/drive/MyDrive/input_files/11.txt\n",
            "Employee_Health_And_Safety\n",
            "/content/drive/MyDrive/input_files/12.txt\n",
            "Business_Ethics\n",
            "/content/drive/MyDrive/input_files/13.txt\n",
            "/content/drive/MyDrive/input_files/14.txt\n",
            "Business_Ethics\n",
            "/content/drive/MyDrive/input_files/15.txt\n",
            "Employee_Health_And_Safety\n",
            "/content/drive/MyDrive/input_files/16.txt\n",
            "Employee_Health_And_Safety\n",
            "/content/drive/MyDrive/input_files/17.txt\n",
            "Management_Of_Legal_And_Regulatory_Framework\n",
            "/content/drive/MyDrive/input_files/18.txt\n",
            "Business_Ethics\n",
            "/content/drive/MyDrive/input_files/19.txt\n",
            "Employee_Health_And_Safety\n",
            "/content/drive/MyDrive/input_files/20.txt\n",
            "Access_And_Affordability\n",
            "/content/drive/MyDrive/input_files/21.txt\n",
            "Labor_Practices\n",
            "/content/drive/MyDrive/input_files/22.txt\n",
            "Data_Security\n",
            "/content/drive/MyDrive/input_files/23.txt\n",
            "Management_Of_Legal_And_Regulatory_Framework\n",
            "/content/drive/MyDrive/input_files/24.txt\n",
            "Product_Design_And_Lifecycle_Management\n",
            "/content/drive/MyDrive/input_files/25.txt\n",
            "Competitive_Behavior\n",
            "/content/drive/MyDrive/input_files/26.txt\n",
            "Competitive_Behavior\n",
            "/content/drive/MyDrive/input_files/27.txt\n",
            "Employee_Health_And_Safety\n",
            "/content/drive/MyDrive/input_files/28.txt\n",
            "Critical_Incident_Risk_Management\n",
            "/content/drive/MyDrive/input_files/29.txt\n",
            "Employee_Health_And_Safety\n",
            "/content/drive/MyDrive/input_files/30.txt\n",
            "Employee_Health_And_Safety\n",
            "/content/drive/MyDrive/input_files/31.txt\n",
            "Employee_Engagement_Inclusion_And_Diversity\n",
            "/content/drive/MyDrive/input_files/32.txt\n",
            "/content/drive/MyDrive/input_files/33.txt\n",
            "/content/drive/MyDrive/input_files/34.txt\n",
            "Business_Ethics\n",
            "/content/drive/MyDrive/input_files/35.txt\n",
            "Employee_Health_And_Safety\n",
            "/content/drive/MyDrive/input_files/36.txt\n",
            "Business_Ethics\n",
            "/content/drive/MyDrive/input_files/37.txt\n",
            "Employee_Health_And_Safety\n",
            "/content/drive/MyDrive/input_files/38.txt\n",
            "Business_Ethics\n",
            "/content/drive/MyDrive/input_files/39.txt\n",
            "Employee_Health_And_Safety\n",
            "/content/drive/MyDrive/input_files/40.txt\n",
            "Labor_Practices\n",
            "/content/drive/MyDrive/input_files/41.txt\n",
            "Business_Ethics\n",
            "/content/drive/MyDrive/input_files/42.txt\n",
            "Employee_Health_And_Safety\n",
            "/content/drive/MyDrive/input_files/43.txt\n",
            "/content/drive/MyDrive/input_files/44.txt\n",
            "Employee_Health_And_Safety\n",
            "/content/drive/MyDrive/input_files/45.txt\n",
            "Business_Ethics\n",
            "/content/drive/MyDrive/input_files/46.txt\n",
            "/content/drive/MyDrive/input_files/47.txt\n",
            "Business_Ethics\n",
            "/content/drive/MyDrive/input_files/48.txt\n",
            "/content/drive/MyDrive/input_files/49.txt\n",
            "Business_Ethics\n",
            "/content/drive/MyDrive/input_files/50.txt\n",
            "/content/drive/MyDrive/input_files/51.txt\n",
            "Data_Security\n",
            "/content/drive/MyDrive/input_files/52.txt\n",
            "Business_Model_Resilience\n",
            "/content/drive/MyDrive/input_files/53.txt\n",
            "Data_Security\n",
            "/content/drive/MyDrive/input_files/54.txt\n",
            "Labor_Practices\n",
            "/content/drive/MyDrive/input_files/55.txt\n",
            "/content/drive/MyDrive/input_files/56.txt\n",
            "Labor_Practices\n",
            "/content/drive/MyDrive/input_files/57.txt\n",
            "/content/drive/MyDrive/input_files/58.txt\n",
            "Energy_Management\n",
            "/content/drive/MyDrive/input_files/59.txt\n",
            "Critical_Incident_Risk_Management\n",
            "/content/drive/MyDrive/input_files/60.txt\n",
            "Business_Ethics\n",
            "/content/drive/MyDrive/input_files/61.txt\n",
            "Labor_Practices\n",
            "/content/drive/MyDrive/input_files/62.txt\n",
            "Employee_Health_And_Safety\n",
            "/content/drive/MyDrive/input_files/63.txt\n",
            "/content/drive/MyDrive/input_files/64.txt\n",
            "Competitive_Behavior\n",
            "/content/drive/MyDrive/input_files/65.txt\n",
            "Critical_Incident_Risk_Management\n",
            "/content/drive/MyDrive/input_files/66.txt\n",
            "Employee_Health_And_Safety\n",
            "/content/drive/MyDrive/input_files/67.txt\n",
            "Systemic_Risk_Management\n",
            "/content/drive/MyDrive/input_files/68.txt\n",
            "/content/drive/MyDrive/input_files/69.txt\n",
            "Data_Security\n",
            "/content/drive/MyDrive/input_files/70.txt\n",
            "Business_Ethics\n",
            "/content/drive/MyDrive/input_files/71.txt\n",
            "Employee_Health_And_Safety\n",
            "/content/drive/MyDrive/input_files/72.txt\n",
            "Data_Security\n",
            "/content/drive/MyDrive/input_files/73.txt\n",
            "Management_Of_Legal_And_Regulatory_Framework\n",
            "/content/drive/MyDrive/input_files/74.txt\n",
            "Employee_Engagement_Inclusion_And_Diversity\n",
            "/content/drive/MyDrive/input_files/75.txt\n",
            "Business_Ethics\n",
            "/content/drive/MyDrive/input_files/76.txt\n",
            "/content/drive/MyDrive/input_files/77.txt\n",
            "Management_Of_Legal_And_Regulatory_Framework\n",
            "/content/drive/MyDrive/input_files/78.txt\n",
            "/content/drive/MyDrive/input_files/79.txt\n",
            "Labor_Practices\n",
            "/content/drive/MyDrive/input_files/80.txt\n",
            "/content/drive/MyDrive/input_files/81.txt\n",
            "Business_Ethics\n",
            "/content/drive/MyDrive/input_files/82.txt\n",
            "Business_Ethics\n",
            "/content/drive/MyDrive/input_files/83.txt\n",
            "Employee_Engagement_Inclusion_And_Diversity\n",
            "/content/drive/MyDrive/input_files/84.txt\n",
            "/content/drive/MyDrive/input_files/85.txt\n",
            "/content/drive/MyDrive/input_files/86.txt\n",
            "GHG_Emissions\n",
            "/content/drive/MyDrive/input_files/87.txt\n",
            "Business_Ethics\n",
            "/content/drive/MyDrive/input_files/88.txt\n",
            "Business_Ethics\n",
            "/content/drive/MyDrive/input_files/89.txt\n",
            "Data_Security\n",
            "/content/drive/MyDrive/input_files/90.txt\n",
            "Employee_Health_And_Safety\n",
            "/content/drive/MyDrive/input_files/91.txt\n",
            "Competitive_Behavior\n",
            "/content/drive/MyDrive/input_files/92.txt\n",
            "Labor_Practices\n",
            "/content/drive/MyDrive/input_files/93.txt\n",
            "Employee_Engagement_Inclusion_And_Diversity\n",
            "/content/drive/MyDrive/input_files/94.txt\n",
            "Management_Of_Legal_And_Regulatory_Framework\n",
            "/content/drive/MyDrive/input_files/95.txt\n",
            "Business_Ethics\n",
            "/content/drive/MyDrive/input_files/96.txt\n",
            "Systemic_Risk_Management\n",
            "/content/drive/MyDrive/input_files/97.txt\n",
            "Business_Ethics\n",
            "/content/drive/MyDrive/input_files/98.txt\n",
            "/content/drive/MyDrive/input_files/99.txt\n",
            "Employee_Engagement_Inclusion_And_Diversity\n",
            "/content/drive/MyDrive/input_files/100.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "```\n",
        "# This is formatted as code\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "s7c9YMMn0_fk"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1M63QiH_DuIurA_boe-fsz8-59IRoSvqN",
      "authorship_tag": "ABX9TyNZWDH9FlHs71xFDcraAUrb",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}